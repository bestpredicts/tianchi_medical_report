{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stretch-worse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:29:01.977287Z",
     "start_time": "2021-03-30T00:29:00.326598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "328 105\n",
      "693 106\n",
      "698 107\n",
      "380 108\n",
      "415 109\n",
      "177 110\n",
      "381 111\n",
      "809 112\n",
      "623 113\n",
      "858\n",
      "963\n",
      "21128\n",
      "963\n",
      "20165\n",
      "[PAD]\n",
      "[unused1]\n",
      "[unused2]\n",
      "[unused3]\n",
      "[unused4]\n",
      "[unused5]\n",
      "[unused6]\n",
      "[unused7]\n",
      "[unused8]\n",
      "[unused9]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import torch\n",
    "\n",
    "train_path = \"../../input/track1_round1_train_20210222.csv\"\n",
    "test_path = \"../../input/track1_round1_testA_20210222.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_path,sep=',',header=None)\n",
    "test_df = pd.read_csv(test_path,sep=',',header=None)\n",
    "\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "train_df[1]=train_df[1].apply(lambda x:x[1:-1])\n",
    "train_df[2]=train_df[2].apply(lambda x:x[1:])\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "test_df[1] = test_df[1].apply(lambda x:x[1:-1])\n",
    "test_df.head()\n",
    "\n",
    "\n",
    "\n",
    "all_df = pd.concat([train_df,test_df])\n",
    "all_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "un_wr_coount =0\n",
    "\n",
    "with open(\"all_data.txt\",'w') as f:\n",
    "    for  l in range(len(all_df)):\n",
    "        text2 = all_df[1].values[l]\n",
    "        text = text2.split(\"693\")\n",
    "        t_len = len(text)\n",
    "        \n",
    "        if len(text2.split())<3:\n",
    "            continue\n",
    "        \n",
    "        if t_len == 1:\n",
    "            f.write(text2+\"\\n\")\n",
    "            f.write(text2+\"\\n\")\n",
    "            f.write(\"\"+\"\\n\")\n",
    "        else:\n",
    "            m = [i for i in range(t_len)]\n",
    "            random.shuffle(m)\n",
    "            m = m[0]\n",
    "            \n",
    "            first = \" 693 \".join(text[:m])\n",
    "            second = \" 693 \".join(text[m:])\n",
    "            if len(first)>1 and len(first.split())>1  and len(second)>1 and len(second.split())>1:\n",
    "                f.write(first+\"\\n\")\n",
    "                f.write(second+\"\\n\")\n",
    "                f.write(\"\"+\"\\n\")\n",
    "            elif len(text2)>1 and len(text2.split())>1:\n",
    "                f.write(text2+\"\\n\")\n",
    "                f.write(text2+\"\\n\")\n",
    "                f.write(\"\"+\"\\n\")\n",
    "            else:\n",
    "                un_wr_coount+=1\n",
    "\n",
    "print(un_wr_coount)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "    \n",
    "dic={}\n",
    "sep_list = ['[PAD]',\n",
    " '[unused1]',\n",
    " '[unused2]',\n",
    " '[unused3]',\n",
    " '[unused4]',\n",
    " '[unused5]',\n",
    " '[unused6]',\n",
    " '[unused7]',\n",
    " '[unused8]',\n",
    " '[unused9]',\n",
    " '[unused10]',\n",
    " '[unused11]',\n",
    " '[unused12]',\n",
    " '[unused13]',\n",
    " '[unused14]',\n",
    " '[unused15]',\n",
    " '[unused16]',\n",
    " '[unused17]',\n",
    " '[unused18]',\n",
    " '[unused19]',\n",
    " '[unused20]',\n",
    " '[unused21]',\n",
    " '[unused22]',\n",
    " '[unused23]',\n",
    " '[unused24]',\n",
    " '[unused25]',\n",
    " '[unused26]',\n",
    " '[unused27]',\n",
    " '[unused28]',\n",
    " '[unused29]',\n",
    " '[unused30]',\n",
    " '[unused31]',\n",
    " '[unused32]',\n",
    " '[unused33]',\n",
    " '[unused34]',\n",
    " '[unused35]',\n",
    " '[unused36]',\n",
    " '[unused37]',\n",
    " '[unused38]',\n",
    " '[unused39]',\n",
    " '[unused40]',\n",
    " '[unused41]',\n",
    " '[unused42]',\n",
    " '[unused43]',\n",
    " '[unused44]',\n",
    " '[unused45]',\n",
    " '[unused46]',\n",
    " '[unused47]',\n",
    " '[unused48]',\n",
    " '[unused49]',\n",
    " '[unused50]',\n",
    " '[unused51]',\n",
    " '[unused52]',\n",
    " '[unused53]',\n",
    " '[unused54]',\n",
    " '[unused55]',\n",
    " '[unused56]',\n",
    " '[unused57]',\n",
    " '[unused58]',\n",
    " '[unused59]',\n",
    " '[unused60]',\n",
    " '[unused61]',\n",
    " '[unused62]',\n",
    " '[unused63]',\n",
    " '[unused64]',\n",
    " '[unused65]',\n",
    " '[unused66]',\n",
    " '[unused67]',\n",
    " '[unused68]',\n",
    " '[unused69]',\n",
    " '[unused70]',\n",
    " '[unused71]',\n",
    " '[unused72]',\n",
    " '[unused73]',\n",
    " '[unused74]',\n",
    " '[unused75]',\n",
    " '[unused76]',\n",
    " '[unused77]',\n",
    " '[unused78]',\n",
    " '[unused79]',\n",
    " '[unused80]',\n",
    " '[unused81]',\n",
    " '[unused82]',\n",
    " '[unused83]',\n",
    " '[unused84]',\n",
    " '[unused85]',\n",
    " '[unused86]',\n",
    " '[unused87]',\n",
    " '[unused88]',\n",
    " '[unused89]',\n",
    " '[unused90]',\n",
    " '[unused91]',\n",
    " '[unused92]',\n",
    " '[unused93]',\n",
    " '[unused94]',\n",
    " '[unused95]',\n",
    " '[unused96]',\n",
    " '[unused97]',\n",
    " '[unused98]',\n",
    " '[unused99]',\n",
    " '[UNK]',\n",
    " '[CLS]',\n",
    " '[SEP]',\n",
    " '[MASK]',\n",
    " '<S>']\n",
    "\n",
    "\n",
    "for idx,token in enumerate(sep_list):\n",
    "     dic[token]= idx\n",
    "\n",
    "# #添加 保证词表大小和bert一样大 而已\n",
    "# for i in range(20706,21128):\n",
    "#     dic[str(i]=i\n",
    "# print(len(dic))\n",
    "\n",
    "\n",
    "conter=Counter()\n",
    "\n",
    "with open(\"all_data.txt\",'r') as f:\n",
    "    text = f.readlines()\n",
    "    for idx,txt in enumerate(text):\n",
    "        # if idx>100:\n",
    "        #     break\n",
    "        for char in txt.split():\n",
    "            conter[char]+=1\n",
    "\n",
    "most_common=conter.most_common(1000000) \n",
    "\n",
    "cont=0\n",
    "for idx,x in enumerate(most_common):\n",
    "    if x[1]>0:\n",
    "        dic[x[0]]=len(dic)\n",
    "    cont+=1\n",
    "    if cont<10:\n",
    "        print(x[0],dic[x[0]])\n",
    "print(cont)\n",
    "\n",
    "\n",
    "import json \n",
    "\n",
    "try:\n",
    "    os.mkdir(\"bert_token\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dic = sorted(dic.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "print(len(dic))\n",
    "\n",
    "# BERT词频\n",
    "import json\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "MODEL_PATH = '/home/root1/DY/nezha-cn-base'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(tokenizer.vocab))\n",
    "print(len(dic))\n",
    "diff = len(tokenizer.vocab)-len(dic)\n",
    "print(diff)\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"bert_token\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "with open(\"bert_token/vocab.txt\",'w') as f:\n",
    "    for idx,key in enumerate(dic):\n",
    "        f.write(key[0]+u\"\\n\")\n",
    "        if idx<10:\n",
    "            print(key[0])\n",
    "        \n",
    "    for i in range(422):\n",
    "        \n",
    "        f.write(\"[unused%d]\\n\"%i)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert_token')\n",
    "\n",
    "from transformers import BertConfig\n",
    "from configuration_nezha import NeZhaConfig\n",
    "from modeling_nezha import NeZhaForMaskedLM\n",
    "\n",
    "\n",
    "BertConfig = NeZhaConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# from transformers import BertForMaskedLM\n",
    "\n",
    "    \n",
    "\n",
    "# model = NeZhaForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "# model.bert.word_embedding=torch.nn.Embedding(len(tokenizer.vocab),768) \n",
    "# from transformers import LineByLineTextDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oriented-millennium",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:29:33.936893Z",
     "start_time": "2021-03-30T00:29:33.795490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623 328 538 382 399 400 478 842 698 137 492 266 521 177 415 381  693  700 132 706 317 534 830 290 512 729 327 548 520 445 51 240 711 818 445 358 240 711 \r\n",
      " 623 328 380 172 54 175 563 470 609 \r\n",
      "\r\n",
      "48 328 538 382 809 623 434 355 382 382 363 145 424 389 \r\n",
      " 808 266 751 335 832 47  693  583 328 305 206 461 204 48 328 740 204 411 204 549 728 832 122 \r\n",
      "\r\n",
      "623 656 293 851 636 842 698 493 338 266 369 691 \r\n",
      " 380 136 363 399 556 698 66 432 449 177 830 381 332 290 380 26 343 28 177 415 832 14 \r\n",
      "\r\n",
      "48 328 380 259 439 107 380 265 172 470 290  693  556 698 54 623 34 138 351 761 \r\n"
     ]
    }
   ],
   "source": [
    "!head -10 all_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-writing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
