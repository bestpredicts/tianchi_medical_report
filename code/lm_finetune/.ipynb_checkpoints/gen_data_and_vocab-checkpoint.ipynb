{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stretch-worse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T12:51:20.888857Z",
     "start_time": "2021-03-29T12:50:38.727835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "171185\n",
      "4496 new duplicates\n",
      "914 new different\n",
      "(100000, 5)\n",
      "(105410, 5)\n",
      "(130410, 5)\n",
      "(25000, 3)\n",
      "find 865\n",
      "find 1102\n",
      "find 1742\n",
      "find 2638\n",
      "find 2902\n",
      "find 2986\n",
      "find 3144\n",
      "find 3854\n",
      "find 3085\n",
      "find 4165\n",
      "find 4180\n",
      "find 4389\n",
      "find 4609\n",
      "find 4918\n",
      "find 5112\n",
      "find 5261\n",
      "find 5558\n",
      "find 5729\n",
      "find 5751\n",
      "find 5981\n",
      "find 6149\n",
      "find 7060\n",
      "find 7208\n",
      "find 7358\n",
      "find 3255\n",
      "find 7555\n",
      "find 7676\n",
      "find 4179\n",
      "find 8247\n",
      "find 8348\n",
      "find 8517\n",
      "find 8556\n",
      "find 8585\n",
      "find 8600\n",
      "find 2436\n",
      "find 8676\n",
      "find 8933\n",
      "find 4342\n",
      "find 2324\n",
      "find 9777\n",
      "find 9781\n",
      "find 9908\n",
      "find 9936\n",
      "find 9682\n",
      "find 10073\n",
      "find 10102\n",
      "find 10291\n",
      "find 10320\n",
      "find 10334\n",
      "find 2292\n",
      "find 1106\n",
      "find 10722\n",
      "find 10818\n",
      "find 10976\n",
      "find 11103\n",
      "find 11207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/root1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/home/root1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:142: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 11217\n",
      "find 11237\n",
      "find 4134\n",
      "find 1148\n",
      "find 11664\n",
      "find 11865\n",
      "find 11899\n",
      "find 11912\n",
      "find 11923\n",
      "find 12017\n",
      "find 8257\n",
      "find 12763\n",
      "find 12992\n",
      "find 13073\n",
      "find 13226\n",
      "find 9481\n",
      "find 11477\n",
      "find 8984\n",
      "find 13373\n",
      "find 13431\n",
      "find 9750\n",
      "find 13575\n",
      "find 13739\n",
      "find 13859\n",
      "find 6342\n",
      "find 14017\n",
      "find 14084\n",
      "find 14098\n",
      "find 14163\n",
      "find 13345\n",
      "find 14418\n",
      "find 11103\n",
      "find 14689\n",
      "find 14692\n",
      "find 13982\n",
      "find 14814\n",
      "find 8298\n",
      "find 9926\n",
      "find 3162\n",
      "find 14932\n",
      "find 15027\n",
      "find 9510\n",
      "find 15100\n",
      "find 7235\n",
      "find 1791\n",
      "find 15301\n",
      "find 15337\n",
      "find 11537\n",
      "find 9443\n",
      "find 15461\n",
      "find 15561\n",
      "find 8092\n",
      "find 15672\n",
      "find 10126\n",
      "find 15265\n",
      "find 5374\n",
      "find 15884\n",
      "find 2072\n",
      "find 15936\n",
      "find 11754\n",
      "find 16034\n",
      "find 12866\n",
      "find 16272\n",
      "find 4179\n",
      "find 15474\n",
      "find 16432\n",
      "find 7323\n",
      "find 10603\n",
      "find 15319\n",
      "find 1183\n",
      "find 2213\n",
      "find 16604\n",
      "find 8853\n",
      "find 8558\n",
      "find 15229\n",
      "find 16701\n",
      "find 16741\n",
      "find 2540\n",
      "find 3431\n",
      "find 6900\n",
      "find 16863\n",
      "find 16913\n",
      "find 3347\n",
      "find 16995\n",
      "find 15421\n",
      "find 17065\n",
      "find 6203\n",
      "find 3829\n",
      "find 17262\n",
      "find 12301\n",
      "find 4485\n",
      "find 17399\n",
      "find 17409\n",
      "find 5345\n",
      "find 13154\n",
      "find 17512\n",
      "find 9481\n",
      "find 17525\n",
      "find 17646\n",
      "find 7571\n",
      "find 17793\n",
      "find 17911\n",
      "find 18008\n",
      "find 18019\n",
      "find 18041\n",
      "find 13073\n",
      "find 11065\n",
      "find 18193\n",
      "find 18270\n",
      "find 18419\n",
      "find 18473\n",
      "find 18514\n",
      "find 18250\n",
      "find 1183\n",
      "find 18539\n",
      "find 15430\n",
      "find 18581\n",
      "find 380\n",
      "find 10714\n",
      "find 7684\n",
      "find 18761\n",
      "find 11722\n",
      "find 7523\n",
      "find 3431\n",
      "find 18854\n",
      "find 5374\n",
      "find 18887\n",
      "find 4179\n",
      "find 9510\n",
      "find 19079\n",
      "find 9926\n",
      "find 4844\n",
      "find 19506\n",
      "find 1818\n",
      "find 13373\n",
      "find 21305\n",
      "find 3872\n",
      "find 21665\n",
      "find 21391\n",
      "find 21870\n",
      "find 20428\n",
      "find 19802\n",
      "find 13073\n",
      "find 5374\n",
      "find 13646\n",
      "find 19174\n",
      "find 16264\n",
      "find 4179\n",
      "find 3564\n",
      "find 6203\n",
      "find 18250\n",
      "find 19218\n",
      "find 5658\n",
      "find 20587\n",
      "find 12457\n",
      "find 17911\n",
      "find 19123\n",
      "find 3015\n",
      "find 3557\n",
      "find 21319\n",
      "find 4890\n",
      "find 16084\n",
      "find 1308\n",
      "find 4553\n",
      "find 20771\n",
      "find 3854\n",
      "find 19084\n",
      "find 21039\n",
      "find 15655\n",
      "find 20076\n",
      "find 20446\n",
      "find 19748\n",
      "find 21729\n",
      "12 105\n",
      "29 106\n",
      "19 107\n",
      "23 108\n",
      "16 109\n",
      "11 110\n",
      "10 111\n",
      "9 112\n",
      "32 113\n",
      "20480\n",
      "20585\n",
      "21128\n",
      "20585\n",
      "543\n",
      "[PAD]\n",
      "[unused1]\n",
      "[unused2]\n",
      "[unused3]\n",
      "[unused4]\n",
      "[unused5]\n",
      "[unused6]\n",
      "[unused7]\n",
      "[unused8]\n",
      "[unused9]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import torch\n",
    "\n",
    "# all_df.head()\n",
    "\n",
    "# all_df_2= all_df.copy()\n",
    "\n",
    "# all_df['text'] = '[CLS] '+all_df['text1']+\" [SEP] \"+all_df['text2']+\" [SEP] \"\n",
    "\n",
    "# all_df_2['text'] = '[CLS] '+all_df_2['text2']+\" [SEP] \"+all_df_2['text1']+\" [SEP] \"\n",
    "\n",
    "# all_df = pd.concat([all_df,all_df_2])\n",
    "# print(all_df.shape)\n",
    "\n",
    "with open(\"all_data.txt\",'w') as f:\n",
    "    for  l in range(len(all_df)):\n",
    "        text1 = all_df['text1'].values[l]\n",
    "        text2 = all_df['text2'].values[l]\n",
    "        \n",
    "        if len(text1)>1 and len(text1.split())>1:\n",
    "            f.write(text1+\"\\n\")\n",
    "            f.write(text2+\"\\n\")\n",
    "            f.write(\"\"+\"\\n\")\n",
    "            \n",
    "            f.write(text2+\"\\n\")\n",
    "            f.write(text1+\"\\n\")\n",
    "            f.write(\"\"+\"\\n\")\n",
    "        else:\n",
    "            print(\"find\",text1)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "    \n",
    "dic={}\n",
    "sep_list = ['[PAD]',\n",
    " '[unused1]',\n",
    " '[unused2]',\n",
    " '[unused3]',\n",
    " '[unused4]',\n",
    " '[unused5]',\n",
    " '[unused6]',\n",
    " '[unused7]',\n",
    " '[unused8]',\n",
    " '[unused9]',\n",
    " '[unused10]',\n",
    " '[unused11]',\n",
    " '[unused12]',\n",
    " '[unused13]',\n",
    " '[unused14]',\n",
    " '[unused15]',\n",
    " '[unused16]',\n",
    " '[unused17]',\n",
    " '[unused18]',\n",
    " '[unused19]',\n",
    " '[unused20]',\n",
    " '[unused21]',\n",
    " '[unused22]',\n",
    " '[unused23]',\n",
    " '[unused24]',\n",
    " '[unused25]',\n",
    " '[unused26]',\n",
    " '[unused27]',\n",
    " '[unused28]',\n",
    " '[unused29]',\n",
    " '[unused30]',\n",
    " '[unused31]',\n",
    " '[unused32]',\n",
    " '[unused33]',\n",
    " '[unused34]',\n",
    " '[unused35]',\n",
    " '[unused36]',\n",
    " '[unused37]',\n",
    " '[unused38]',\n",
    " '[unused39]',\n",
    " '[unused40]',\n",
    " '[unused41]',\n",
    " '[unused42]',\n",
    " '[unused43]',\n",
    " '[unused44]',\n",
    " '[unused45]',\n",
    " '[unused46]',\n",
    " '[unused47]',\n",
    " '[unused48]',\n",
    " '[unused49]',\n",
    " '[unused50]',\n",
    " '[unused51]',\n",
    " '[unused52]',\n",
    " '[unused53]',\n",
    " '[unused54]',\n",
    " '[unused55]',\n",
    " '[unused56]',\n",
    " '[unused57]',\n",
    " '[unused58]',\n",
    " '[unused59]',\n",
    " '[unused60]',\n",
    " '[unused61]',\n",
    " '[unused62]',\n",
    " '[unused63]',\n",
    " '[unused64]',\n",
    " '[unused65]',\n",
    " '[unused66]',\n",
    " '[unused67]',\n",
    " '[unused68]',\n",
    " '[unused69]',\n",
    " '[unused70]',\n",
    " '[unused71]',\n",
    " '[unused72]',\n",
    " '[unused73]',\n",
    " '[unused74]',\n",
    " '[unused75]',\n",
    " '[unused76]',\n",
    " '[unused77]',\n",
    " '[unused78]',\n",
    " '[unused79]',\n",
    " '[unused80]',\n",
    " '[unused81]',\n",
    " '[unused82]',\n",
    " '[unused83]',\n",
    " '[unused84]',\n",
    " '[unused85]',\n",
    " '[unused86]',\n",
    " '[unused87]',\n",
    " '[unused88]',\n",
    " '[unused89]',\n",
    " '[unused90]',\n",
    " '[unused91]',\n",
    " '[unused92]',\n",
    " '[unused93]',\n",
    " '[unused94]',\n",
    " '[unused95]',\n",
    " '[unused96]',\n",
    " '[unused97]',\n",
    " '[unused98]',\n",
    " '[unused99]',\n",
    " '[UNK]',\n",
    " '[CLS]',\n",
    " '[SEP]',\n",
    " '[MASK]',\n",
    " '<S>']\n",
    "\n",
    "\n",
    "for idx,token in enumerate(sep_list):\n",
    "     dic[token]= idx\n",
    "\n",
    "# #添加 保证词表大小和bert一样大 而已\n",
    "# for i in range(20706,21128):\n",
    "#     dic[str(i]=i\n",
    "# print(len(dic))\n",
    "\n",
    "\n",
    "conter=Counter()\n",
    "\n",
    "with open(\"all_data.txt\",'r') as f:\n",
    "    text = f.readlines()\n",
    "    for idx,txt in enumerate(text):\n",
    "        # if idx>100:\n",
    "        #     break\n",
    "        for char in txt.split():\n",
    "            conter[char]+=1\n",
    "\n",
    "most_common=conter.most_common(1000000) \n",
    "\n",
    "cont=0\n",
    "for idx,x in enumerate(most_common):\n",
    "    if x[1]>0:\n",
    "        dic[x[0]]=len(dic)\n",
    "    cont+=1\n",
    "    if cont<10:\n",
    "        print(x[0],dic[x[0]])\n",
    "print(cont)\n",
    "\n",
    "\n",
    "import json \n",
    "\n",
    "try:\n",
    "    os.mkdir(\"bert_token\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dic = sorted(dic.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "print(len(dic))\n",
    "\n",
    "# BERT词频\n",
    "import json\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "MODEL_PATH = '/home/root1/DY/nezha-cn-base'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(tokenizer.vocab))\n",
    "print(len(dic))\n",
    "diff = len(tokenizer.vocab)-len(dic)\n",
    "print(diff)\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"bert_token\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "with open(\"bert_token/vocab.txt\",'w') as f:\n",
    "    for idx,key in enumerate(dic):\n",
    "        f.write(key[0]+u\"\\n\")\n",
    "        if idx<10:\n",
    "            print(key[0])\n",
    "        \n",
    "    for i in range(422):\n",
    "        \n",
    "        f.write(\"[unused%d]\\n\"%i)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert_token')\n",
    "\n",
    "from transformers import BertConfig\n",
    "from configuration_nezha import NeZhaConfig\n",
    "from modeling_nezha import NeZhaForMaskedLM\n",
    "\n",
    "\n",
    "BertConfig = NeZhaConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# from transformers import BertForMaskedLM\n",
    "\n",
    "    \n",
    "\n",
    "# model = NeZhaForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "# model.bert.word_embedding=torch.nn.Embedding(len(tokenizer.vocab),768) \n",
    "# from transformers import LineByLineTextDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-millennium",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
