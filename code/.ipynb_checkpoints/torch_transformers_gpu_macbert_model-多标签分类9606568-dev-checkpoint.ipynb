{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:30.582036Z",
     "start_time": "2021-03-27T00:51:30.578337Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39911,
     "status": "ok",
     "timestamp": 1614770541554,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "MmMivBRyDbtO",
    "outputId": "35305dcd-c1d0-454c-a173-e8042a4efc30"
   },
   "outputs": [],
   "source": [
    "#  #colab setting \n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import os\n",
    "# os.chdir('/content/drive/My Drive/Competitions/医学影像报告异常检测/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:32.892864Z",
     "start_time": "2021-03-27T00:51:30.585454Z"
    },
    "executionInfo": {
     "elapsed": 54243,
     "status": "ok",
     "timestamp": 1614770555888,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "9yok51vCDoWl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:32.939978Z",
     "start_time": "2021-03-27T00:51:32.894921Z"
    },
    "executionInfo": {
     "elapsed": 54246,
     "status": "ok",
     "timestamp": 1614770555896,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "VpFla5NVPayv"
   },
   "outputs": [],
   "source": [
    "train_path = \"../input/track1_round1_train_20210222.csv\"\n",
    "test_path = \"../input/track1_round1_testA_20210222.csv\"\n",
    "train_df = pd.read_csv(train_path,sep=',',header=None)\n",
    "test_df = pd.read_csv(test_path,sep=',',header=None)\n",
    "\n",
    "train_df[1]=train_df[1].apply(lambda x:x[1:-1])\n",
    "train_df[2]=train_df[2].apply(lambda x:x[1:])\n",
    "\n",
    "test_df[1] = test_df[1].apply(lambda x:x[1:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:32.950131Z",
     "start_time": "2021-03-27T00:51:32.941580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "train_df.drop_duplicates(subset=[1,2],inplace=True)\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:32.963266Z",
     "start_time": "2021-03-27T00:51:32.951485Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "executionInfo": {
     "elapsed": 54241,
     "status": "ok",
     "timestamp": 1614770555897,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "f1GQc7JFDz46",
    "outputId": "d7609944-c200-4d73-8906-f31cb906c7c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>623 328 538 382 399 400 478 842 698 137 492 26...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>48 328 538 382 809 623 434 355 382 382 363 145...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>623 656 293 851 636 842 698 493 338 266 369 69...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>48 328 380 259 439 107 380 265 172 470 290 693...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>623 328 399 698 493 338 266 14 177 415 511 647...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5|</td>\n",
       "      <td>80 328 328 54 172 439 741 380 172 842 698 177 ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6|</td>\n",
       "      <td>48 322 795 856 374 439 48 328 443 380 597 172 ...</td>\n",
       "      <td>4 11 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7|</td>\n",
       "      <td>623 328 659 486 582 162 711 289 606 405 809 78...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8|</td>\n",
       "      <td>852 328 471 585 117 458 399 607 693 380 522 62...</td>\n",
       "      <td>6 12 14 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9|</td>\n",
       "      <td>229 172 200 737 437 547 651 693 623 328 355 65...</td>\n",
       "      <td>1 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10|</td>\n",
       "      <td>852 328 305 461 71 413 728 479 122 693 697 382...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11|</td>\n",
       "      <td>697 582 439 48 328 755 355 582 617 265 478 162...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12|</td>\n",
       "      <td>380 315 177 415 145 755 693 698 521 177 415 38...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13|</td>\n",
       "      <td>811 328 538 845 832 122 693 788 579 460 787 95...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14|</td>\n",
       "      <td>623 328 697 661 809 48 46 355 661 414 852 328 ...</td>\n",
       "      <td>0 1 8 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15|</td>\n",
       "      <td>111 328 213 661 542 265 363 145 424 490 693 66...</td>\n",
       "      <td>10 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16|</td>\n",
       "      <td>543 328 328 380 197 320 698 160 338 14 177 415...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17|</td>\n",
       "      <td>380 172 551 737 221 290 480 171 514 569 231 11...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18|</td>\n",
       "      <td>623 328 204 461 851 842 698 549 81 832 122 693...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19|</td>\n",
       "      <td>358 380 616 363 399 556 698 313 66 432 449 133...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text        label\n",
       "0     0|  623 328 538 382 399 400 478 842 698 137 492 26...           2 \n",
       "1     1|  48 328 538 382 809 623 434 355 382 382 363 145...             \n",
       "2     2|  623 656 293 851 636 842 698 493 338 266 369 69...          15 \n",
       "3     3|  48 328 380 259 439 107 380 265 172 470 290 693...             \n",
       "4     4|  623 328 399 698 493 338 266 14 177 415 511 647...          16 \n",
       "5     5|  80 328 328 54 172 439 741 380 172 842 698 177 ...          15 \n",
       "6     6|  48 322 795 856 374 439 48 328 443 380 597 172 ...     4 11 15 \n",
       "7     7|  623 328 659 486 582 162 711 289 606 405 809 78...             \n",
       "8     8|  852 328 471 585 117 458 399 607 693 380 522 62...  6 12 14 15 \n",
       "9     9|  229 172 200 737 437 547 651 693 623 328 355 65...         1 3 \n",
       "10   10|  852 328 305 461 71 413 728 479 122 693 697 382...             \n",
       "11   11|  697 582 439 48 328 755 355 582 617 265 478 162...          15 \n",
       "12   12|  380 315 177 415 145 755 693 698 521 177 415 38...          14 \n",
       "13   13|  811 328 538 845 832 122 693 788 579 460 787 95...             \n",
       "14   14|  623 328 697 661 809 48 46 355 661 414 852 328 ...    0 1 8 10 \n",
       "15   15|  111 328 213 661 542 265 363 145 424 490 693 66...       10 15 \n",
       "16   16|  543 328 328 380 197 320 698 160 338 14 177 415...          15 \n",
       "17   17|  380 172 551 737 221 290 480 171 514 569 231 11...             \n",
       "18   18|  623 328 204 461 851 842 698 549 81 832 122 693...          16 \n",
       "19   19|  358 380 616 363 399 556 698 313 66 432 449 133...             "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns = ['index','text','label']\n",
    "\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:33.058896Z",
     "start_time": "2021-03-27T00:51:32.965199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>623 328 538 382 399 400 478 842 698 137 492 26...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>48 328 538 382 809 623 434 355 382 382 363 145...</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>623 656 293 851 636 842 698 493 338 266 369 69...</td>\n",
       "      <td>15</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>48 328 380 259 439 107 380 265 172 470 290 693...</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>623 328 399 698 493 338 266 14 177 415 511 647...</td>\n",
       "      <td>16</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                               text label  \\\n",
       "0    0|  623 328 538 382 399 400 478 842 698 137 492 26...    2    \n",
       "1    1|  48 328 538 382 809 623 434 355 382 382 363 145...         \n",
       "2    2|  623 656 293 851 636 842 698 493 338 266 369 69...   15    \n",
       "3    3|  48 328 380 259 439 107 380 265 172 470 290 693...         \n",
       "4    4|  623 328 399 698 493 338 266 14 177 415 511 647...   16    \n",
       "\n",
       "                                              label2  \n",
       "0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dumm(s):\n",
    "    re=[0]*17\n",
    "    if s=='' or s==\" \":\n",
    "        return re\n",
    "    else:\n",
    "        tmp=[int(i) for i in s.split(' ') if i!='']\n",
    "        for i in tmp:\n",
    "            re[i]=1\n",
    "    return re\n",
    "\n",
    "train_df['label2'] = train_df.label.apply(lambda x:get_dumm(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:33.110349Z",
     "start_time": "2021-03-27T00:51:33.060403Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(17):\n",
    "    train_df['label_onehot_%d'%(i+1)] = train_df.label2.apply(lambda x:x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:33.127949Z",
     "start_time": "2021-03-27T00:51:33.112773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "      <th>label_onehot_1</th>\n",
       "      <th>label_onehot_2</th>\n",
       "      <th>label_onehot_3</th>\n",
       "      <th>label_onehot_4</th>\n",
       "      <th>label_onehot_5</th>\n",
       "      <th>label_onehot_6</th>\n",
       "      <th>...</th>\n",
       "      <th>label_onehot_8</th>\n",
       "      <th>label_onehot_9</th>\n",
       "      <th>label_onehot_10</th>\n",
       "      <th>label_onehot_11</th>\n",
       "      <th>label_onehot_12</th>\n",
       "      <th>label_onehot_13</th>\n",
       "      <th>label_onehot_14</th>\n",
       "      <th>label_onehot_15</th>\n",
       "      <th>label_onehot_16</th>\n",
       "      <th>label_onehot_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>623 328 538 382 399 400 478 842 698 137 492 26...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>48 328 538 382 809 623 434 355 382 382 363 145...</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>623 656 293 851 636 842 698 493 338 266 369 69...</td>\n",
       "      <td>15</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>48 328 380 259 439 107 380 265 172 470 290 693...</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>623 328 399 698 493 338 266 14 177 415 511 647...</td>\n",
       "      <td>16</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                               text label  \\\n",
       "0    0|  623 328 538 382 399 400 478 842 698 137 492 26...    2    \n",
       "1    1|  48 328 538 382 809 623 434 355 382 382 363 145...         \n",
       "2    2|  623 656 293 851 636 842 698 493 338 266 369 69...   15    \n",
       "3    3|  48 328 380 259 439 107 380 265 172 470 290 693...         \n",
       "4    4|  623 328 399 698 493 338 266 14 177 415 511 647...   16    \n",
       "\n",
       "                                              label2  label_onehot_1  \\\n",
       "0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "\n",
       "   label_onehot_2  label_onehot_3  label_onehot_4  label_onehot_5  \\\n",
       "0               0               1               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   label_onehot_6  ...  label_onehot_8  label_onehot_9  label_onehot_10  \\\n",
       "0               0  ...               0               0                0   \n",
       "1               0  ...               0               0                0   \n",
       "2               0  ...               0               0                0   \n",
       "3               0  ...               0               0                0   \n",
       "4               0  ...               0               0                0   \n",
       "\n",
       "   label_onehot_11  label_onehot_12  label_onehot_13  label_onehot_14  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   label_onehot_15  label_onehot_16  label_onehot_17  \n",
       "0                0                0                0  \n",
       "1                0                0                0  \n",
       "2                0                1                0  \n",
       "3                0                0                0  \n",
       "4                0                0                1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:33.136470Z",
     "start_time": "2021-03-27T00:51:33.130249Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 54233,
     "status": "ok",
     "timestamp": 1614770555898,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "ESC_TLmo236c",
    "outputId": "c6dbcd8b-794b-49bc-a0fc-b4d7b8dfab44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>852 328 697 538 142 355 582 800 728 4 647 169 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>380 358 343 654 171 832 47 832 690 693 48 563 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>751 335 834 582 717 583 585 693 623 328 107 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>623 328 649 582 488 12 578 623 538 382 382 265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>83 293 398 797 382 363 145 424 693 698 800 691...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                               text\n",
       "0    0|  852 328 697 538 142 355 582 800 728 4 647 169 ...\n",
       "1    1|  380 358 343 654 171 832 47 832 690 693 48 563 ...\n",
       "2    2|  751 335 834 582 717 583 585 693 623 328 107 38...\n",
       "3    3|  623 328 649 582 488 12 578 623 538 382 382 265...\n",
       "4    4|  83 293 398 797 382 363 145 424 693 698 800 691..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns = ['index','text']\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:40.912425Z",
     "start_time": "2021-03-27T00:51:33.138482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label_onehot_1_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_1_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_1_word_ctr_list_min_fea</th>\n",
       "      <th>label_onehot_1_word_ctr_list_std_fea</th>\n",
       "      <th>label_onehot_1_word_ctr_list_sum_fea</th>\n",
       "      <th>label_onehot_2_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_2_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_2_word_ctr_list_min_fea</th>\n",
       "      <th>...</th>\n",
       "      <th>label_onehot_16_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_min_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_std_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_sum_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_min_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_std_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_sum_fea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>852 328 697 538 142 355 582 800 728 4 647 169 ...</td>\n",
       "      <td>0.124919</td>\n",
       "      <td>0.393008</td>\n",
       "      <td>0.054608</td>\n",
       "      <td>0.079149</td>\n",
       "      <td>4.122316</td>\n",
       "      <td>0.127946</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.058020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167549</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.089904</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>5.529130</td>\n",
       "      <td>0.077433</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.038760</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>2.555291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>380 358 343 654 171 832 47 832 690 693 48 563 ...</td>\n",
       "      <td>0.112956</td>\n",
       "      <td>0.197067</td>\n",
       "      <td>0.079536</td>\n",
       "      <td>0.021437</td>\n",
       "      <td>3.501624</td>\n",
       "      <td>0.108086</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.064725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205120</td>\n",
       "      <td>0.303226</td>\n",
       "      <td>0.159265</td>\n",
       "      <td>0.034133</td>\n",
       "      <td>6.358730</td>\n",
       "      <td>0.087905</td>\n",
       "      <td>0.156806</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>2.725062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>751 335 834 582 717 583 585 693 623 328 107 38...</td>\n",
       "      <td>0.124938</td>\n",
       "      <td>0.224359</td>\n",
       "      <td>0.062696</td>\n",
       "      <td>0.039191</td>\n",
       "      <td>2.373818</td>\n",
       "      <td>0.131235</td>\n",
       "      <td>0.224359</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199417</td>\n",
       "      <td>0.566116</td>\n",
       "      <td>0.100313</td>\n",
       "      <td>0.092820</td>\n",
       "      <td>3.788926</td>\n",
       "      <td>0.093853</td>\n",
       "      <td>0.241986</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.044474</td>\n",
       "      <td>1.783205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>623 328 649 582 488 12 578 623 538 382 382 265...</td>\n",
       "      <td>0.130126</td>\n",
       "      <td>0.423272</td>\n",
       "      <td>0.052124</td>\n",
       "      <td>0.051302</td>\n",
       "      <td>8.067832</td>\n",
       "      <td>0.131398</td>\n",
       "      <td>0.338954</td>\n",
       "      <td>0.046332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203535</td>\n",
       "      <td>0.492278</td>\n",
       "      <td>0.089904</td>\n",
       "      <td>0.069743</td>\n",
       "      <td>12.619198</td>\n",
       "      <td>0.075658</td>\n",
       "      <td>0.152263</td>\n",
       "      <td>0.015444</td>\n",
       "      <td>0.025709</td>\n",
       "      <td>4.690812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>83 293 398 797 382 363 145 424 693 698 800 691...</td>\n",
       "      <td>0.122920</td>\n",
       "      <td>0.230356</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.041414</td>\n",
       "      <td>5.039707</td>\n",
       "      <td>0.137787</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.071305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176610</td>\n",
       "      <td>0.331435</td>\n",
       "      <td>0.054931</td>\n",
       "      <td>0.057410</td>\n",
       "      <td>7.241030</td>\n",
       "      <td>0.078502</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.017427</td>\n",
       "      <td>0.023886</td>\n",
       "      <td>3.218583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                               text  \\\n",
       "0    0|  852 328 697 538 142 355 582 800 728 4 647 169 ...   \n",
       "1    1|  380 358 343 654 171 832 47 832 690 693 48 563 ...   \n",
       "2    2|  751 335 834 582 717 583 585 693 623 328 107 38...   \n",
       "3    3|  623 328 649 582 488 12 578 623 538 382 382 265...   \n",
       "4    4|  83 293 398 797 382 363 145 424 693 698 800 691...   \n",
       "\n",
       "   label_onehot_1_word_ctr_list_mean_fea  \\\n",
       "0                               0.124919   \n",
       "1                               0.112956   \n",
       "2                               0.124938   \n",
       "3                               0.130126   \n",
       "4                               0.122920   \n",
       "\n",
       "   label_onehot_1_word_ctr_list_max_fea  label_onehot_1_word_ctr_list_min_fea  \\\n",
       "0                              0.393008                              0.054608   \n",
       "1                              0.197067                              0.079536   \n",
       "2                              0.224359                              0.062696   \n",
       "3                              0.423272                              0.052124   \n",
       "4                              0.230356                              0.045000   \n",
       "\n",
       "   label_onehot_1_word_ctr_list_std_fea  label_onehot_1_word_ctr_list_sum_fea  \\\n",
       "0                              0.079149                              4.122316   \n",
       "1                              0.021437                              3.501624   \n",
       "2                              0.039191                              2.373818   \n",
       "3                              0.051302                              8.067832   \n",
       "4                              0.041414                              5.039707   \n",
       "\n",
       "   label_onehot_2_word_ctr_list_mean_fea  \\\n",
       "0                               0.127946   \n",
       "1                               0.108086   \n",
       "2                               0.131235   \n",
       "3                               0.131398   \n",
       "4                               0.137787   \n",
       "\n",
       "   label_onehot_2_word_ctr_list_max_fea  label_onehot_2_word_ctr_list_min_fea  \\\n",
       "0                              0.403226                              0.058020   \n",
       "1                              0.158730                              0.064725   \n",
       "2                              0.224359                              0.078652   \n",
       "3                              0.338954                              0.046332   \n",
       "4                              0.403226                              0.071305   \n",
       "\n",
       "   ...  label_onehot_16_word_ctr_list_mean_fea  \\\n",
       "0  ...                                0.167549   \n",
       "1  ...                                0.205120   \n",
       "2  ...                                0.199417   \n",
       "3  ...                                0.203535   \n",
       "4  ...                                0.176610   \n",
       "\n",
       "   label_onehot_16_word_ctr_list_max_fea  \\\n",
       "0                               0.285714   \n",
       "1                               0.303226   \n",
       "2                               0.566116   \n",
       "3                               0.492278   \n",
       "4                               0.331435   \n",
       "\n",
       "   label_onehot_16_word_ctr_list_min_fea  \\\n",
       "0                               0.089904   \n",
       "1                               0.159265   \n",
       "2                               0.100313   \n",
       "3                               0.089904   \n",
       "4                               0.054931   \n",
       "\n",
       "   label_onehot_16_word_ctr_list_std_fea  \\\n",
       "0                               0.042000   \n",
       "1                               0.034133   \n",
       "2                               0.092820   \n",
       "3                               0.069743   \n",
       "4                               0.057410   \n",
       "\n",
       "   label_onehot_16_word_ctr_list_sum_fea  \\\n",
       "0                               5.529130   \n",
       "1                               6.358730   \n",
       "2                               3.788926   \n",
       "3                              12.619198   \n",
       "4                               7.241030   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_mean_fea  \\\n",
       "0                                0.077433   \n",
       "1                                0.087905   \n",
       "2                                0.093853   \n",
       "3                                0.075658   \n",
       "4                                0.078502   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_max_fea  \\\n",
       "0                               0.147368   \n",
       "1                               0.156806   \n",
       "2                               0.241986   \n",
       "3                               0.152263   \n",
       "4                               0.147368   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_min_fea  \\\n",
       "0                               0.038760   \n",
       "1                               0.041667   \n",
       "2                               0.029197   \n",
       "3                               0.015444   \n",
       "4                               0.017427   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_std_fea  \\\n",
       "0                               0.028777   \n",
       "1                               0.030928   \n",
       "2                               0.044474   \n",
       "3                               0.025709   \n",
       "4                               0.023886   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_sum_fea  \n",
       "0                               2.555291  \n",
       "1                               2.725062  \n",
       "2                               1.783205  \n",
       "3                               4.690812  \n",
       "4                               3.218583  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_words_probs(df_a,df_b,label_name):\n",
    "    df_b = df_b.copy()\n",
    "    m_data = df_a.copy()\n",
    "    m_data['text'] = m_data['text'].apply(lambda x:x.split())\n",
    "    \n",
    "    word_ctr = {}\n",
    "    word_count = {}\n",
    "\n",
    "    for idx,i in enumerate(range(len(m_data))):\n",
    "        words = m_data.text.values[i]\n",
    "        words = [i for i in set(words)]\n",
    "        label = m_data[label_name].values[i]\n",
    "        for w in words:\n",
    "            word_count[w]=word_count.get(w,0)+1\n",
    "            if label==1 :  #词频大于3\n",
    "                word_ctr[w]=word_ctr.get(w,0)+1\n",
    "                \n",
    "    for k  in  word_ctr.keys():\n",
    "        word_ctr[k] = word_ctr[k]/word_count[k]\n",
    "\n",
    "    df_b['text'] =  df_b['text'].apply(lambda x:x.split())\n",
    "    df_b['word_count_list'] = df_b.text.apply(lambda x:[word_count.get(i,0) for i in x ])\n",
    "    df_b['word_ctr_list'] = df_b.text.apply(lambda x:[word_ctr.get(i,0) for i in x ])\n",
    "\n",
    "    import numpy as np\n",
    "    df_b['%s_word_ctr_list_mean_fea'%label_name] = df_b['word_ctr_list'].apply(lambda x:np.mean(x))\n",
    "    df_b['%s_word_ctr_list_max_fea'%label_name] = df_b['word_ctr_list'].apply(lambda x:np.max(x))\n",
    "    df_b['%s_word_ctr_list_min_fea'%label_name] = df_b['word_ctr_list'].apply(lambda x:np.min(x))\n",
    "    df_b['%s_word_ctr_list_std_fea'%label_name] = df_b['word_ctr_list'].apply(lambda x:np.std(x))\n",
    "    df_b['%s_word_ctr_list_sum_fea'%label_name] = df_b['word_ctr_list'].apply(lambda x:np.sum(x))\n",
    "    df_b['text'] =  df_b['text'].apply(lambda x:\" \".join(x))\n",
    "    \n",
    "    del df_b['word_count_list'],df_b['word_ctr_list']\n",
    "\n",
    "    return df_b\n",
    "\n",
    "for i in range(17):\n",
    "    label_name = 'label_onehot_%d'%(i+1)\n",
    "    test_df = get_words_probs(train_df,test_df,label_name)\n",
    "    \n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:40.919456Z",
     "start_time": "2021-03-27T00:51:40.915156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "['label_onehot_1_word_ctr_list_mean_fea', 'label_onehot_1_word_ctr_list_max_fea', 'label_onehot_1_word_ctr_list_min_fea', 'label_onehot_1_word_ctr_list_std_fea', 'label_onehot_1_word_ctr_list_sum_fea', 'label_onehot_2_word_ctr_list_mean_fea', 'label_onehot_2_word_ctr_list_max_fea', 'label_onehot_2_word_ctr_list_min_fea', 'label_onehot_2_word_ctr_list_std_fea', 'label_onehot_2_word_ctr_list_sum_fea', 'label_onehot_3_word_ctr_list_mean_fea', 'label_onehot_3_word_ctr_list_max_fea', 'label_onehot_3_word_ctr_list_min_fea', 'label_onehot_3_word_ctr_list_std_fea', 'label_onehot_3_word_ctr_list_sum_fea', 'label_onehot_4_word_ctr_list_mean_fea', 'label_onehot_4_word_ctr_list_max_fea', 'label_onehot_4_word_ctr_list_min_fea', 'label_onehot_4_word_ctr_list_std_fea', 'label_onehot_4_word_ctr_list_sum_fea', 'label_onehot_5_word_ctr_list_mean_fea', 'label_onehot_5_word_ctr_list_max_fea', 'label_onehot_5_word_ctr_list_min_fea', 'label_onehot_5_word_ctr_list_std_fea', 'label_onehot_5_word_ctr_list_sum_fea', 'label_onehot_6_word_ctr_list_mean_fea', 'label_onehot_6_word_ctr_list_max_fea', 'label_onehot_6_word_ctr_list_min_fea', 'label_onehot_6_word_ctr_list_std_fea', 'label_onehot_6_word_ctr_list_sum_fea', 'label_onehot_7_word_ctr_list_mean_fea', 'label_onehot_7_word_ctr_list_max_fea', 'label_onehot_7_word_ctr_list_min_fea', 'label_onehot_7_word_ctr_list_std_fea', 'label_onehot_7_word_ctr_list_sum_fea', 'label_onehot_8_word_ctr_list_mean_fea', 'label_onehot_8_word_ctr_list_max_fea', 'label_onehot_8_word_ctr_list_min_fea', 'label_onehot_8_word_ctr_list_std_fea', 'label_onehot_8_word_ctr_list_sum_fea', 'label_onehot_9_word_ctr_list_mean_fea', 'label_onehot_9_word_ctr_list_max_fea', 'label_onehot_9_word_ctr_list_min_fea', 'label_onehot_9_word_ctr_list_std_fea', 'label_onehot_9_word_ctr_list_sum_fea', 'label_onehot_10_word_ctr_list_mean_fea', 'label_onehot_10_word_ctr_list_max_fea', 'label_onehot_10_word_ctr_list_min_fea', 'label_onehot_10_word_ctr_list_std_fea', 'label_onehot_10_word_ctr_list_sum_fea', 'label_onehot_11_word_ctr_list_mean_fea', 'label_onehot_11_word_ctr_list_max_fea', 'label_onehot_11_word_ctr_list_min_fea', 'label_onehot_11_word_ctr_list_std_fea', 'label_onehot_11_word_ctr_list_sum_fea', 'label_onehot_12_word_ctr_list_mean_fea', 'label_onehot_12_word_ctr_list_max_fea', 'label_onehot_12_word_ctr_list_min_fea', 'label_onehot_12_word_ctr_list_std_fea', 'label_onehot_12_word_ctr_list_sum_fea', 'label_onehot_13_word_ctr_list_mean_fea', 'label_onehot_13_word_ctr_list_max_fea', 'label_onehot_13_word_ctr_list_min_fea', 'label_onehot_13_word_ctr_list_std_fea', 'label_onehot_13_word_ctr_list_sum_fea', 'label_onehot_14_word_ctr_list_mean_fea', 'label_onehot_14_word_ctr_list_max_fea', 'label_onehot_14_word_ctr_list_min_fea', 'label_onehot_14_word_ctr_list_std_fea', 'label_onehot_14_word_ctr_list_sum_fea', 'label_onehot_15_word_ctr_list_mean_fea', 'label_onehot_15_word_ctr_list_max_fea', 'label_onehot_15_word_ctr_list_min_fea', 'label_onehot_15_word_ctr_list_std_fea', 'label_onehot_15_word_ctr_list_sum_fea', 'label_onehot_16_word_ctr_list_mean_fea', 'label_onehot_16_word_ctr_list_max_fea', 'label_onehot_16_word_ctr_list_min_fea', 'label_onehot_16_word_ctr_list_std_fea', 'label_onehot_16_word_ctr_list_sum_fea', 'label_onehot_17_word_ctr_list_mean_fea', 'label_onehot_17_word_ctr_list_max_fea', 'label_onehot_17_word_ctr_list_min_fea', 'label_onehot_17_word_ctr_list_std_fea', 'label_onehot_17_word_ctr_list_sum_fea']\n"
     ]
    }
   ],
   "source": [
    "features = [i for i in test_df.columns if i not in ['index','text']]\n",
    "print(len(features))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:51:41.323610Z",
     "start_time": "2021-03-27T00:51:40.921621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/root1/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "skf = model_selection.StratifiedKFold(n_splits=5, random_state=4590, shuffle=True)\n",
    "for fold, (trn_, val_) in enumerate(skf.split(X=train_df, y=train_df.label.values)):\n",
    "    train_df.loc[val_, 'kfold'] = fold\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:09.753976Z",
     "start_time": "2021-03-27T00:51:41.325843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "      <th>label_onehot_1</th>\n",
       "      <th>label_onehot_2</th>\n",
       "      <th>label_onehot_3</th>\n",
       "      <th>label_onehot_4</th>\n",
       "      <th>label_onehot_5</th>\n",
       "      <th>label_onehot_6</th>\n",
       "      <th>...</th>\n",
       "      <th>label_onehot_16_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_min_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_std_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_sum_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_min_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_std_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_sum_fea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>623 328 538 382 399 400 478 842 698 137 492 26...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164440</td>\n",
       "      <td>0.316399</td>\n",
       "      <td>0.053208</td>\n",
       "      <td>0.061719</td>\n",
       "      <td>7.893109</td>\n",
       "      <td>0.069383</td>\n",
       "      <td>0.153386</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.029972</td>\n",
       "      <td>3.330386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>623 328 399 698 493 338 266 14 177 415 511 647...</td>\n",
       "      <td>16</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189166</td>\n",
       "      <td>0.334732</td>\n",
       "      <td>0.069841</td>\n",
       "      <td>0.060036</td>\n",
       "      <td>4.161647</td>\n",
       "      <td>0.084684</td>\n",
       "      <td>0.129310</td>\n",
       "      <td>0.040923</td>\n",
       "      <td>0.023791</td>\n",
       "      <td>1.863055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5|</td>\n",
       "      <td>80 328 328 54 172 439 741 380 172 842 698 177 ...</td>\n",
       "      <td>15</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195058</td>\n",
       "      <td>0.316399</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.054437</td>\n",
       "      <td>7.022088</td>\n",
       "      <td>0.079598</td>\n",
       "      <td>0.135647</td>\n",
       "      <td>0.038522</td>\n",
       "      <td>0.024237</td>\n",
       "      <td>2.865540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7|</td>\n",
       "      <td>623 328 659 486 582 162 711 289 606 405 809 78...</td>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180168</td>\n",
       "      <td>0.398872</td>\n",
       "      <td>0.078740</td>\n",
       "      <td>0.061147</td>\n",
       "      <td>7.747219</td>\n",
       "      <td>0.082768</td>\n",
       "      <td>0.135647</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.028315</td>\n",
       "      <td>3.559021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14|</td>\n",
       "      <td>623 328 697 661 809 48 46 355 661 414 852 328 ...</td>\n",
       "      <td>0 1 8 10</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187897</td>\n",
       "      <td>0.325301</td>\n",
       "      <td>0.088875</td>\n",
       "      <td>0.056875</td>\n",
       "      <td>7.891675</td>\n",
       "      <td>0.065703</td>\n",
       "      <td>0.147959</td>\n",
       "      <td>0.020346</td>\n",
       "      <td>0.026922</td>\n",
       "      <td>2.759523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text      label  \\\n",
       "0     0|  623 328 538 382 399 400 478 842 698 137 492 26...         2    \n",
       "4     4|  623 328 399 698 493 338 266 14 177 415 511 647...        16    \n",
       "5     5|  80 328 328 54 172 439 741 380 172 842 698 177 ...        15    \n",
       "7     7|  623 328 659 486 582 162 711 289 606 405 809 78...              \n",
       "14   14|  623 328 697 661 809 48 46 355 661 414 852 328 ...  0 1 8 10    \n",
       "\n",
       "                                               label2  label_onehot_1  \\\n",
       "0   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "5   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...               0   \n",
       "14  [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...               1   \n",
       "\n",
       "    label_onehot_2  label_onehot_3  label_onehot_4  label_onehot_5  \\\n",
       "0                0               1               0               0   \n",
       "4                0               0               0               0   \n",
       "5                0               0               0               0   \n",
       "7                0               0               0               0   \n",
       "14               1               0               0               0   \n",
       "\n",
       "    label_onehot_6  ...  label_onehot_16_word_ctr_list_mean_fea  \\\n",
       "0                0  ...                                0.164440   \n",
       "4                0  ...                                0.189166   \n",
       "5                0  ...                                0.195058   \n",
       "7                0  ...                                0.180168   \n",
       "14               0  ...                                0.187897   \n",
       "\n",
       "    label_onehot_16_word_ctr_list_max_fea  \\\n",
       "0                                0.316399   \n",
       "4                                0.334732   \n",
       "5                                0.316399   \n",
       "7                                0.398872   \n",
       "14                               0.325301   \n",
       "\n",
       "    label_onehot_16_word_ctr_list_min_fea  \\\n",
       "0                                0.053208   \n",
       "4                                0.069841   \n",
       "5                                0.064815   \n",
       "7                                0.078740   \n",
       "14                               0.088875   \n",
       "\n",
       "    label_onehot_16_word_ctr_list_std_fea  \\\n",
       "0                                0.061719   \n",
       "4                                0.060036   \n",
       "5                                0.054437   \n",
       "7                                0.061147   \n",
       "14                               0.056875   \n",
       "\n",
       "    label_onehot_16_word_ctr_list_sum_fea  \\\n",
       "0                                7.893109   \n",
       "4                                4.161647   \n",
       "5                                7.022088   \n",
       "7                                7.747219   \n",
       "14                               7.891675   \n",
       "\n",
       "    label_onehot_17_word_ctr_list_mean_fea  \\\n",
       "0                                 0.069383   \n",
       "4                                 0.084684   \n",
       "5                                 0.079598   \n",
       "7                                 0.082768   \n",
       "14                                0.065703   \n",
       "\n",
       "    label_onehot_17_word_ctr_list_max_fea  \\\n",
       "0                                0.153386   \n",
       "4                                0.129310   \n",
       "5                                0.135647   \n",
       "7                                0.135647   \n",
       "14                               0.147959   \n",
       "\n",
       "    label_onehot_17_word_ctr_list_min_fea  \\\n",
       "0                                0.009756   \n",
       "4                                0.040923   \n",
       "5                                0.038522   \n",
       "7                                0.029412   \n",
       "14                               0.020346   \n",
       "\n",
       "    label_onehot_17_word_ctr_list_std_fea  \\\n",
       "0                                0.029972   \n",
       "4                                0.023791   \n",
       "5                                0.024237   \n",
       "7                                0.028315   \n",
       "14                               0.026922   \n",
       "\n",
       "    label_onehot_17_word_ctr_list_sum_fea  \n",
       "0                                3.330386  \n",
       "4                                1.863055  \n",
       "5                                2.865540  \n",
       "7                                3.559021  \n",
       "14                               2.759523  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_df = []\n",
    "\n",
    "for fold in range(5):\n",
    "    df_a = train_df[train_df.kfold!=fold]\n",
    "    df_b = train_df[train_df.kfold==fold]\n",
    "    for i in range(17):\n",
    "        label_name = 'label_onehot_%d'%(i+1)\n",
    "        df_b = get_words_probs(df_a,df_b,label_name)\n",
    "    new_train_df.append(df_b)\n",
    "    \n",
    "train_df = pd.concat(new_train_df,axis=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:09.758554Z",
     "start_time": "2021-03-27T00:52:09.755973Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:09.814751Z",
     "start_time": "2021-03-27T00:52:09.760458Z"
    },
    "executionInfo": {
     "elapsed": 54231,
     "status": "ok",
     "timestamp": 1614770555900,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "AP7PUpB6FjDo"
   },
   "outputs": [],
   "source": [
    "def data_aug(df):\n",
    "    \n",
    "    import random\n",
    "    df = df.copy()\n",
    "    df[2]=df[2].apply(lambda x:x.split())\n",
    "    new_df = df.copy()\n",
    "    m =  shuffle(df)\n",
    "    m.reset_index(drop=True,inplace=True)\n",
    "    new_df[0]=100000\n",
    "    new_df[1] = new_df[1]+m[1]\n",
    "    new_df[2] = new_df[2]+m[2]\n",
    "    new_df[2]=new_df[2].apply(lambda x:\" \".join(list(set(x))))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ3gBoKgvOMd"
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:09.860072Z",
     "start_time": "2021-03-27T00:52:09.817079Z"
    },
    "executionInfo": {
     "elapsed": 54229,
     "status": "ok",
     "timestamp": 1614770555900,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "01VWaoAiFsbE"
   },
   "outputs": [],
   "source": [
    "# 建立分词器\n",
    "\n",
    "import transformers\n",
    "\n",
    "class config:\n",
    "    DEVICE = \"cuda\"\n",
    "    MAX_LEN = 200\n",
    "    TRAIN_BATCH_SIZE = 30\n",
    "    VALID_BATCH_SIZE = 30\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    EPOCHS = 10\n",
    "    BERT_PATH = \"new_bert\"\n",
    "    TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n",
    "\n",
    "\n",
    "save_path = config.BERT_PATH+'_weight_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUcAWkrbpmWI"
   },
   "source": [
    "## SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.017889Z",
     "start_time": "2021-03-27T00:52:09.861715Z"
    },
    "executionInfo": {
     "elapsed": 54226,
     "status": "ok",
     "timestamp": 1614770555900,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "Erl9-n_upohh"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import torch  \n",
    "import numpy as np\n",
    "import os\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Wra6TtssOb"
   },
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.032393Z",
     "start_time": "2021-03-27T00:52:10.019598Z"
    },
    "executionInfo": {
     "elapsed": 54225,
     "status": "ok",
     "timestamp": 1614770555901,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "JteBD9p1oxxp"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BERTDataset:\n",
    "    def __init__(self,num_features,text1,text2, target):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.target = target\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_length = config.MAX_LEN\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text1)\n",
    "    def get_dumm(self,s):\n",
    "        # print(\"print s\",s)\n",
    "        # print(\"s split\",s.split(' '))\n",
    "\n",
    "        re=[0]*17\n",
    "        if s=='' or s==\" \":\n",
    "            return re\n",
    "        else:\n",
    "            tmp=[int(i) for i in s.split(' ') if i!='']\n",
    "            for i in tmp:\n",
    "                re[i]=1\n",
    "        return re\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text1 = str(self.text1[item])\n",
    "        text1 = \" \".join(text1.split())\n",
    "        text2 = None\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            text2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        mask_padding_with_zero=True\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "        pad_on_left = False\n",
    "        pad_token=0\n",
    "        pad_token_segment_id=0\n",
    "\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
    "            token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
    "        else:\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([pad_token_segment_id] )* padding_length\n",
    "                                               \n",
    "        label = self.target[item]\n",
    "        # print(\"label\",label)\n",
    "        #print(label,[i for i in label])\n",
    "        label=self.get_dumm(label)\n",
    "#         print(label)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(label, dtype=torch.float),\n",
    "            \"num_features\":torch.from_numpy(self.num_features[item]).float()\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "# class BERTDataset:\n",
    "#     def __init__(self,text1,text2, target):\n",
    "#         self.text1 = text1\n",
    "#         self.text2 = text2\n",
    "#         self.target = target\n",
    "#         self.tokenizer = config.TOKENIZER\n",
    "#         self.max_length = config.MAX_LEN\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.text1)\n",
    "\n",
    "#     def get_dumm(self,s):\n",
    "#         # print(\"print s\",s)\n",
    "#         # print(\"s split\",s.split(' '))\n",
    "\n",
    "#         re=[0]*17\n",
    "#         if s=='' or s==\" \":\n",
    "#             return re\n",
    "#         else:\n",
    "#             tmp=[int(i) for i in s.split(' ') if i!='']\n",
    "#             for i in tmp:\n",
    "#                 re[i]=1\n",
    "#         return re\n",
    "\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         text1 = str(self.text1[item])\n",
    "#         text1 = \" \".join(text1.split())\n",
    "#         text2 = None\n",
    "#         # text2 = str(self.text2[item])\n",
    "#         # text2 = \" \".join(text2.split())\n",
    "\n",
    "#         # inputs = self.tokenizer.encode_plus(\n",
    "#         #     text1,\n",
    "#         #     text2,\n",
    "#         #     # add_special_tokens=True,\n",
    "#         #     max_length=self.max_len,\n",
    "#         #     pad_to_max_length=True,\n",
    "#         #     truncation=True\n",
    "#         # )\n",
    "\n",
    "#         # ids = inputs[\"input_ids\"]\n",
    "#         # mask = inputs[\"attention_mask\"]\n",
    "#         # token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "#         inputs = self.tokenizer.encode_plus(\n",
    "#             text1,\n",
    "#             text2,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=self.max_length,\n",
    "#             truncate_first_sequence=True  # We're truncating the first sequence in priority\n",
    "#         )\n",
    "#         input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "#         # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "#         # tokens are attended to.\n",
    "#         mask_padding_with_zero=True\n",
    "#         attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "#         # Zero-pad up to the sequence length.\n",
    "#         padding_length = self.max_length - len(input_ids)\n",
    "#         pad_on_left = False\n",
    "#         pad_token=0\n",
    "#         pad_token_segment_id=0\n",
    "\n",
    "#         if pad_on_left:\n",
    "#             input_ids = ([pad_token] * padding_length) + input_ids\n",
    "#             attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
    "#             token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
    "#         else:\n",
    "#             input_ids = input_ids + ([pad_token] * padding_length)\n",
    "#             attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "#             token_type_ids = token_type_ids + ([pad_token_segment_id] )* padding_length\n",
    "                                               \n",
    "#         label = self.target[item]\n",
    "#         # print(\"label\",label)\n",
    "#         #print(label,[i for i in label])\n",
    "#         label=self.get_dumm(label)\n",
    "\n",
    "\n",
    "#         return {\n",
    "#             \"ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "#             \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "#             \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "#             \"targets\": torch.tensor(label, dtype=torch.float),\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54222,
     "status": "ok",
     "timestamp": 1614770555901,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "hOZTjExbUVZP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.071149Z",
     "start_time": "2021-03-27T00:52:10.033917Z"
    },
    "executionInfo": {
     "elapsed": 54221,
     "status": "ok",
     "timestamp": 1614770555902,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "AlmOklJ1JPb7"
   },
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel,BertModel\n",
    "# from transformers.modeling_roberta import RobertaModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSl04MfxtWup"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.086171Z",
     "start_time": "2021-03-27T00:52:10.072856Z"
    },
    "executionInfo": {
     "elapsed": 54219,
     "status": "ok",
     "timestamp": 1614770555902,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "CJQFASGKswVR"
   },
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomBert(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        config.output_hidden_states = True\n",
    "        config.num_labels = 2\n",
    "\n",
    "        super(CustomBert, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = transformers.BertModel(config)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = config.num_hidden_layers + 1+13\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "#         self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "        self.one_classifier = nn.Linear(256, 17)\n",
    "    \n",
    "        self.line2 = nn.Linear(768+17,256)\n",
    "\n",
    "\n",
    "        self.dp1 = nn.Dropout(0.4)\n",
    "        self.bn1 = nn.BatchNorm1d(17)\n",
    "        self.bn2 = nn.BatchNorm1d(768+17)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                num_features=None\n",
    "                ):\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids\n",
    "                            )\n",
    "\n",
    "        hidden_layers = outputs[2]\n",
    "        last_layers = outputs[0]\n",
    "#         print(len(hidden_layers))\n",
    "\n",
    "        add_emb =[ torch.mean(layer,dim=1) for layer in hidden_layers] #14\n",
    "        \n",
    "        cls_outputs = torch.stack([self.dropout(layer[:, 0, :]) for layer in hidden_layers]+add_emb,\n",
    "                                  dim=2)\n",
    "#         print(cls_outputs.size())\n",
    "#         print(self.layer_weights.size())\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0) * cls_outputs).sum(-1)\n",
    "        \n",
    "#         linear_out = F.relu(self.line1(num_features))\n",
    "#         linear_out = self.dp1(linear_out)\n",
    "#         linear_out = self.bn1(linear_out)\n",
    "        \n",
    "\n",
    "        cls_output =  torch.cat((cls_output,num_features),axis=1)\n",
    "        cls_output = self.bn2(cls_output)\n",
    "        \n",
    "        cls_output = F.relu(self.line2(cls_output))\n",
    "        cls_output = self.bn3(cls_output)\n",
    "        \n",
    "\n",
    "        \n",
    "            # multisample dropout (wut): https://arxiv.org/abs/1905.09788\n",
    "        logits2 = torch.mean(torch.stack([\n",
    "            self.one_classifier(self.high_dropout(cls_output))\n",
    "            for _ in range(5)\n",
    "        ], dim=0), dim=0)\n",
    "        \n",
    "\n",
    "        return logits2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.094241Z",
     "start_time": "2021-03-27T00:52:10.087751Z"
    },
    "executionInfo": {
     "elapsed": 54217,
     "status": "ok",
     "timestamp": 1614770555903,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "DxTdZUYgwr57"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FGM():\n",
    "    \"\"\"\n",
    "    对抗训练方法类\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=0.5, emb_name='word_embeddings'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s5vuD5dtqRJ"
   },
   "source": [
    "## train units && eval units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.110708Z",
     "start_time": "2021-03-27T00:52:10.096363Z"
    },
    "executionInfo": {
     "elapsed": 54215,
     "status": "ok",
     "timestamp": 1614770555903,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "sW_BcT4ltgxX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.BCEWithLogitsLoss()(outputs.view(-1,17), targets.view(-1, 17))\n",
    "\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    fgm = FGM(model)\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets = d[\"targets\"]\n",
    "        num_feas = d['num_features']\n",
    "\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        num_feas = num_feas.to(device, dtype=torch.float)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=ids, attention_mask=mask,token_type_ids=token_type_ids,num_features=num_feas)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        fgm.attack() # 在embedding上添加对抗扰动\n",
    "        outputs = model(input_ids=ids, attention_mask=mask,token_type_ids=token_type_ids,num_features=num_feas)\n",
    "        loss_adv = loss_fn(outputs, targets)\n",
    "        # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "        loss_adv.backward() \n",
    "        # 恢复embedding参数\n",
    "        fgm.restore() \n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"targets\"]\n",
    "            num_feas = d['num_features']\n",
    "\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            num_feas = num_feas.to(device, dtype=torch.float)\n",
    "\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask,token_type_ids=token_type_ids,num_features=num_feas)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.117004Z",
     "start_time": "2021-03-27T00:52:10.114303Z"
    },
    "executionInfo": {
     "elapsed": 54214,
     "status": "ok",
     "timestamp": 1614770555904,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "H1UmKtbj1n-E"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score,accuracy_score,recall_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "# def threshold_search(y_true, y_proba, plot=False):\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "#     thresholds = np.append(thresholds, 1.001) \n",
    "#     F = 2 / (1/precision + 1/recall)\n",
    "#     best_score = np.max(F)\n",
    "#     best_th = thresholds[np.argmax(F)]\n",
    "#     if plot:\n",
    "#         plt.plot(thresholds, F, '-b')\n",
    "#         plt.plot([best_th], [best_score], '*r')\n",
    "#         plt.show()\n",
    "#     search_result = {'threshold': best_th , 'f1': best_score}\n",
    "#     return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.140285Z",
     "start_time": "2021-03-27T00:52:10.119331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label_onehot_1_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_1_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_1_word_ctr_list_min_fea</th>\n",
       "      <th>label_onehot_1_word_ctr_list_std_fea</th>\n",
       "      <th>label_onehot_1_word_ctr_list_sum_fea</th>\n",
       "      <th>label_onehot_2_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_2_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_2_word_ctr_list_min_fea</th>\n",
       "      <th>...</th>\n",
       "      <th>label_onehot_16_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_min_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_std_fea</th>\n",
       "      <th>label_onehot_16_word_ctr_list_sum_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_mean_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_max_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_min_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_std_fea</th>\n",
       "      <th>label_onehot_17_word_ctr_list_sum_fea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>852 328 697 538 142 355 582 800 728 4 647 169 ...</td>\n",
       "      <td>0.124919</td>\n",
       "      <td>0.393008</td>\n",
       "      <td>0.054608</td>\n",
       "      <td>0.079149</td>\n",
       "      <td>4.122316</td>\n",
       "      <td>0.127946</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.058020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167549</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.089904</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>5.529130</td>\n",
       "      <td>0.077433</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.038760</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>2.555291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>380 358 343 654 171 832 47 832 690 693 48 563 ...</td>\n",
       "      <td>0.112956</td>\n",
       "      <td>0.197067</td>\n",
       "      <td>0.079536</td>\n",
       "      <td>0.021437</td>\n",
       "      <td>3.501624</td>\n",
       "      <td>0.108086</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.064725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205120</td>\n",
       "      <td>0.303226</td>\n",
       "      <td>0.159265</td>\n",
       "      <td>0.034133</td>\n",
       "      <td>6.358730</td>\n",
       "      <td>0.087905</td>\n",
       "      <td>0.156806</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>2.725062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>751 335 834 582 717 583 585 693 623 328 107 38...</td>\n",
       "      <td>0.124938</td>\n",
       "      <td>0.224359</td>\n",
       "      <td>0.062696</td>\n",
       "      <td>0.039191</td>\n",
       "      <td>2.373818</td>\n",
       "      <td>0.131235</td>\n",
       "      <td>0.224359</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199417</td>\n",
       "      <td>0.566116</td>\n",
       "      <td>0.100313</td>\n",
       "      <td>0.092820</td>\n",
       "      <td>3.788926</td>\n",
       "      <td>0.093853</td>\n",
       "      <td>0.241986</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.044474</td>\n",
       "      <td>1.783205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>623 328 649 582 488 12 578 623 538 382 382 265...</td>\n",
       "      <td>0.130126</td>\n",
       "      <td>0.423272</td>\n",
       "      <td>0.052124</td>\n",
       "      <td>0.051302</td>\n",
       "      <td>8.067832</td>\n",
       "      <td>0.131398</td>\n",
       "      <td>0.338954</td>\n",
       "      <td>0.046332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203535</td>\n",
       "      <td>0.492278</td>\n",
       "      <td>0.089904</td>\n",
       "      <td>0.069743</td>\n",
       "      <td>12.619198</td>\n",
       "      <td>0.075658</td>\n",
       "      <td>0.152263</td>\n",
       "      <td>0.015444</td>\n",
       "      <td>0.025709</td>\n",
       "      <td>4.690812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>83 293 398 797 382 363 145 424 693 698 800 691...</td>\n",
       "      <td>0.122920</td>\n",
       "      <td>0.230356</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.041414</td>\n",
       "      <td>5.039707</td>\n",
       "      <td>0.137787</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.071305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176610</td>\n",
       "      <td>0.331435</td>\n",
       "      <td>0.054931</td>\n",
       "      <td>0.057410</td>\n",
       "      <td>7.241030</td>\n",
       "      <td>0.078502</td>\n",
       "      <td>0.147368</td>\n",
       "      <td>0.017427</td>\n",
       "      <td>0.023886</td>\n",
       "      <td>3.218583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                               text  \\\n",
       "0    0|  852 328 697 538 142 355 582 800 728 4 647 169 ...   \n",
       "1    1|  380 358 343 654 171 832 47 832 690 693 48 563 ...   \n",
       "2    2|  751 335 834 582 717 583 585 693 623 328 107 38...   \n",
       "3    3|  623 328 649 582 488 12 578 623 538 382 382 265...   \n",
       "4    4|  83 293 398 797 382 363 145 424 693 698 800 691...   \n",
       "\n",
       "   label_onehot_1_word_ctr_list_mean_fea  \\\n",
       "0                               0.124919   \n",
       "1                               0.112956   \n",
       "2                               0.124938   \n",
       "3                               0.130126   \n",
       "4                               0.122920   \n",
       "\n",
       "   label_onehot_1_word_ctr_list_max_fea  label_onehot_1_word_ctr_list_min_fea  \\\n",
       "0                              0.393008                              0.054608   \n",
       "1                              0.197067                              0.079536   \n",
       "2                              0.224359                              0.062696   \n",
       "3                              0.423272                              0.052124   \n",
       "4                              0.230356                              0.045000   \n",
       "\n",
       "   label_onehot_1_word_ctr_list_std_fea  label_onehot_1_word_ctr_list_sum_fea  \\\n",
       "0                              0.079149                              4.122316   \n",
       "1                              0.021437                              3.501624   \n",
       "2                              0.039191                              2.373818   \n",
       "3                              0.051302                              8.067832   \n",
       "4                              0.041414                              5.039707   \n",
       "\n",
       "   label_onehot_2_word_ctr_list_mean_fea  \\\n",
       "0                               0.127946   \n",
       "1                               0.108086   \n",
       "2                               0.131235   \n",
       "3                               0.131398   \n",
       "4                               0.137787   \n",
       "\n",
       "   label_onehot_2_word_ctr_list_max_fea  label_onehot_2_word_ctr_list_min_fea  \\\n",
       "0                              0.403226                              0.058020   \n",
       "1                              0.158730                              0.064725   \n",
       "2                              0.224359                              0.078652   \n",
       "3                              0.338954                              0.046332   \n",
       "4                              0.403226                              0.071305   \n",
       "\n",
       "   ...  label_onehot_16_word_ctr_list_mean_fea  \\\n",
       "0  ...                                0.167549   \n",
       "1  ...                                0.205120   \n",
       "2  ...                                0.199417   \n",
       "3  ...                                0.203535   \n",
       "4  ...                                0.176610   \n",
       "\n",
       "   label_onehot_16_word_ctr_list_max_fea  \\\n",
       "0                               0.285714   \n",
       "1                               0.303226   \n",
       "2                               0.566116   \n",
       "3                               0.492278   \n",
       "4                               0.331435   \n",
       "\n",
       "   label_onehot_16_word_ctr_list_min_fea  \\\n",
       "0                               0.089904   \n",
       "1                               0.159265   \n",
       "2                               0.100313   \n",
       "3                               0.089904   \n",
       "4                               0.054931   \n",
       "\n",
       "   label_onehot_16_word_ctr_list_std_fea  \\\n",
       "0                               0.042000   \n",
       "1                               0.034133   \n",
       "2                               0.092820   \n",
       "3                               0.069743   \n",
       "4                               0.057410   \n",
       "\n",
       "   label_onehot_16_word_ctr_list_sum_fea  \\\n",
       "0                               5.529130   \n",
       "1                               6.358730   \n",
       "2                               3.788926   \n",
       "3                              12.619198   \n",
       "4                               7.241030   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_mean_fea  \\\n",
       "0                                0.077433   \n",
       "1                                0.087905   \n",
       "2                                0.093853   \n",
       "3                                0.075658   \n",
       "4                                0.078502   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_max_fea  \\\n",
       "0                               0.147368   \n",
       "1                               0.156806   \n",
       "2                               0.241986   \n",
       "3                               0.152263   \n",
       "4                               0.147368   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_min_fea  \\\n",
       "0                               0.038760   \n",
       "1                               0.041667   \n",
       "2                               0.029197   \n",
       "3                               0.015444   \n",
       "4                               0.017427   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_std_fea  \\\n",
       "0                               0.028777   \n",
       "1                               0.030928   \n",
       "2                               0.044474   \n",
       "3                               0.025709   \n",
       "4                               0.023886   \n",
       "\n",
       "   label_onehot_17_word_ctr_list_sum_fea  \n",
       "0                               2.555291  \n",
       "1                               2.725062  \n",
       "2                               1.783205  \n",
       "3                               4.690812  \n",
       "4                               3.218583  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.152553Z",
     "start_time": "2021-03-27T00:52:10.142439Z"
    },
    "executionInfo": {
     "elapsed": 54213,
     "status": "ok",
     "timestamp": 1614770555904,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "2Y5yWlJ1JU8T"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_df['label']=''\n",
    "\n",
    "\n",
    "test_num_features = test_df[features].values\n",
    "\n",
    "test_dataset = BERTDataset(num_features=test_num_features,text1=test_df.text.values,text2=None,target=test_df.label.values)\n",
    "\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:52:10.165340Z",
     "start_time": "2021-03-27T00:52:10.154232Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54207,
     "status": "ok",
     "timestamp": 1614770555905,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "3o9ESVHNveHE",
    "outputId": "41172ca1-a855-49cb-eda9-08d5e15b0a5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': tensor([101, 118, 106, 136, 140, 239, 138, 125, 312, 192, 604, 294, 180, 304,\n",
      "        311, 187, 959, 174, 105, 118, 106, 136, 125, 112, 140, 621, 291, 414,\n",
      "        379, 192, 160, 253, 119, 161, 102,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'num_features': tensor([1.2492e-01, 3.9301e-01, 5.4608e-02, 7.9149e-02, 4.1223e+00, 1.2795e-01,\n",
      "        4.0323e-01, 5.8020e-02, 6.8453e-02, 4.2222e+00, 1.2013e-01, 3.8173e-01,\n",
      "        4.7101e-02, 7.3254e-02, 3.9643e+00, 5.3626e-02, 1.1834e-01, 3.0101e-02,\n",
      "        2.0308e-02, 1.7697e+00, 9.8463e-02, 1.8385e-01, 4.4379e-02, 3.6670e-02,\n",
      "        3.2493e+00, 3.4056e-02, 7.2464e-02, 1.1029e-02, 1.5192e-02, 1.1239e+00,\n",
      "        2.0934e-02, 9.8765e-02, 4.4379e-03, 1.6370e-02, 6.9083e-01, 1.3056e-01,\n",
      "        3.8543e-01, 3.0717e-02, 7.3545e-02, 4.3084e+00, 1.2463e-01, 3.7262e-01,\n",
      "        5.8824e-02, 6.1480e-02, 4.1128e+00, 1.1897e-01, 3.3942e-01, 2.0672e-02,\n",
      "        6.6546e-02, 3.9260e+00, 4.6604e-02, 8.6420e-02, 1.9355e-02, 1.7091e-02,\n",
      "        1.5379e+00, 8.4604e-02, 1.6659e-01, 3.1065e-02, 3.4874e-02, 2.7919e+00,\n",
      "        3.9883e-02, 7.4074e-02, 1.0989e-02, 1.6226e-02, 1.3161e+00, 2.1673e-02,\n",
      "        4.6385e-02, 0.0000e+00, 9.2203e-03, 7.1521e-01, 4.7758e-02, 7.1587e-02,\n",
      "        1.4493e-02, 1.5644e-02, 1.5760e+00, 1.6755e-01, 2.8571e-01, 8.9904e-02,\n",
      "        4.2000e-02, 5.5291e+00, 7.7433e-02, 1.4737e-01, 3.8760e-02, 2.8777e-02,\n",
      "        2.5553e+00])}\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.104696Z",
     "start_time": "2021-03-27T00:52:10.167033Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "To_m5aPptg0s",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第 1 折训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at new_bert were not used when initializing CustomBert: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing CustomBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomBert were not initialized from the model checkpoint at new_bert and are newly initialized: ['layer_weights', 'one_classifier.weight', 'one_classifier.bias', 'line1.weight', 'line1.bias', 'line2.weight', 'line2.bias', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn2.weight', 'bn2.bias', 'bn2.running_mean', 'bn2.running_var', 'bn3.weight', 'bn3.bias', 'bn3.running_mean', 'bn3.running_var']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 266/266 [01:38<00:00,  2.71it/s]\n",
      "100%|██████████| 69/69 [00:04<00:00, 16.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.848466157913208, score is 0.950090229511261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [01:40<00:00,  2.66it/s]\n",
      "100%|██████████| 69/69 [00:04<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.8041114211082458, score is 0.9526993036270142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 9/266 [00:03<01:48,  2.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-037311a15a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m#         loss= torch.nn.BCEWithLogitsLoss()(torch.tensor(outputs),torch.tensor(targets))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-e5b50f6028c1>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mfgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 在embedding上添加对抗扰动\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_feas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mloss_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# 反向传播，并在正常的grad基础上，累加对抗训练的梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-275bf674e044>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, num_features)\u001b[0m\n\u001b[1;32m     48\u001b[0m                             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                             )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers-4.5.0.dev0-py3.7.egg/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         )\n\u001b[1;32m    983\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers-4.5.0.dev0-py3.7.egg/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 )\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers-4.5.0.dev0-py3.7.egg/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         )\n\u001b[1;32m    499\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers-4.5.0.dev0-py3.7.egg/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers-4.5.0.dev0-py3.7.egg/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers-4.5.0.dev0-py3.7.egg/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,GroupKFold,StratifiedKFold\n",
    "from sklearn.metrics import precision_score , recall_score\t, roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "# gkf =  GroupKFold(n_splits=5)\n",
    "# groups_by_text_id_list = train['qid'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "oof_df_list = []\n",
    "\n",
    "try:\n",
    "    os.mkdir(save_path)\n",
    "except:\n",
    "    pass \n",
    "\n",
    "oof_train_list = []\n",
    "pred = [] \n",
    "\n",
    "# score\n",
    "\n",
    "def ss_score(targets,outputs):\n",
    "    \n",
    "    targets = torch.tensor(targets,dtype=torch.float)\n",
    "    outputs = torch.tensor(outputs,dtype=torch.float)\n",
    "    loss = nn.BCEWithLogitsLoss()(outputs.view(-1,17), targets.view(-1, 17))\n",
    "    score = 1- loss/17\n",
    "    return loss.cpu().detach().numpy(),score\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    \n",
    "\n",
    "    print(\"开始第 %d 折训练\"%(fold+1))\n",
    "    \n",
    "\n",
    "    \n",
    "    model_path = \"%s/bert_wwm_model_%d.bin\"%(save_path,fold)\n",
    "    config.MODEL_PATH = model_path\n",
    "#     tr_df = train_df.iloc[trn_idx]\n",
    "#     va_df = train_df.iloc[val_idx]\n",
    "\n",
    "    tr_df = train_df[train_df.kfold!=fold]\n",
    "    va_df = train_df[train_df.kfold==fold]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    tr_df_num_feas = tr_df[features].values\n",
    "    va_df_num_feas = va_df[features].values\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = BERTDataset(num_features=tr_df_num_feas,\n",
    "        text1=tr_df.text.values,text2=None,target=tr_df.label.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4,shuffle=True\n",
    "    )\n",
    "\n",
    "    valid_dataset = BERTDataset(num_features=va_df_num_feas,\n",
    "     text1= va_df.text.values,text2=None,target=va_df.label.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
    "    )\n",
    "\n",
    "    device = torch.device(config.DEVICE)\n",
    "    model = CustomBert.from_pretrained(config.BERT_PATH,num_labels=1)\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    # pretrain model param\n",
    "    param_pre = [(n, p) for n, p in param_optimizer if 'bert' in n]\n",
    "    # dym param\n",
    "    param_dym = [(n, p) for n, p in param_optimizer if 'layer_weights' in n ]\n",
    "    # funsion layer param\n",
    "    param_line = [p for n, p in param_optimizer if 'one_classifier' in n or 'line1' in n or 'line2' in n]\n",
    "    # 不进行衰减的权重\n",
    "    \n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "         # pretrain model param\n",
    "        # 衰减\n",
    "        {'params': [p for n, p in param_pre if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.01, 'lr': 2e-5\n",
    "         },\n",
    "        # 不衰减\n",
    "        {'params': [p for n, p in param_pre if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0, 'lr': 2e-5\n",
    "         },\n",
    "        # dym  layer\n",
    "        # 衰减\n",
    "        {'params': [p for n, p in param_dym if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.01, 'lr': 1e-3\n",
    "         },\n",
    "        # 不衰减\n",
    "        {'params': [p for n, p in param_dym if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0, 'lr':1e-3\n",
    "         },\n",
    " \n",
    "        # linear  layer\n",
    "        # 衰减\n",
    "        {'params': [p for p in param_line ],\n",
    "         'weight_decay': 0.0, 'lr': 1e-3\n",
    "         }\n",
    "    ]\n",
    "\n",
    "\n",
    "    num_train_steps = int(len(tr_df) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    warmup_steps = int(num_train_steps*0.1)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=2e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,num_warmup_steps=warmup_steps,num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    min_loss = 100000\n",
    "    best_score =0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        outputs, targets = eval_fn(valid_data_loader, model, device)\n",
    "#         loss= torch.nn.BCEWithLogitsLoss()(torch.tensor(outputs),torch.tensor(targets))\n",
    "        loss,score = ss_score(targets,outputs)\n",
    "\n",
    "        # result =  threshold_search(targets,outputs)\n",
    "        # thr = result['threshold']\n",
    "        # f1 = result['f1']\n",
    "        # outputs = (outputs>thr).astype(int)\n",
    "        # f1 = f1_score(targets,outputs)\n",
    "        # precision = precision_score(targets, outputs)\n",
    "        # recall =  recall_score(targets,outputs)\n",
    "\n",
    "        print(f\"loss = {loss}, score is {score}\")\n",
    "\n",
    "\n",
    "#         if loss<min_loss:\n",
    "#             torch.save(model.state_dict(), config.MODEL_PATH)\n",
    "#             min_loss = loss\n",
    "\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(),config.MODEL_PATH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load( config.MODEL_PATH))\n",
    "    outputs, targets = eval_fn(valid_data_loader, model, device)\n",
    "    va_df['prob'] = outputs\n",
    "    oof_train_list.append(va_df)\n",
    "\n",
    "    outputs, targets = eval_fn(test_data_loader, model, device)\n",
    "    pred.append(np.array(outputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.106627Z",
     "start_time": "2021-03-27T00:51:30.625Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "TVCqxmfaHHro"
   },
   "outputs": [],
   "source": [
    "oof_train = pd.concat(oof_train_list)\n",
    "oof_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.107477Z",
     "start_time": "2021-03-27T00:51:30.626Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "S1svWP3sJPmb"
   },
   "outputs": [],
   "source": [
    "sub = np.average(pred, axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.108279Z",
     "start_time": "2021-03-27T00:51:30.627Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "xMYQOboMMx5a"
   },
   "outputs": [],
   "source": [
    "m = sub.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.109102Z",
     "start_time": "2021-03-27T00:51:30.628Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "lNKLriwIL5YP"
   },
   "outputs": [],
   "source": [
    "def get_dumm(s):\n",
    "    re=[0]*17\n",
    "    if s=='' or s==\" \":\n",
    "        return re\n",
    "    else:\n",
    "        tmp=[int(i) for i in s.split(' ') if i!='']\n",
    "        for i in tmp:\n",
    "            re[i]=1\n",
    "    return re\n",
    "\n",
    "\n",
    "oof_train['label'] = oof_train[2].apply(lambda x:get_dumm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.109980Z",
     "start_time": "2021-03-27T00:51:30.629Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.111025Z",
     "start_time": "2021-03-27T00:51:30.630Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_train['score'] = oof_train.apply(lambda x:ss_score(x.label,x.prob)[1],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.111762Z",
     "start_time": "2021-03-27T00:51:30.632Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_score = oof_train.score.mean()\n",
    "print(oof_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.112635Z",
     "start_time": "2021-03-27T00:51:30.633Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "h7Mv4gKCDBfx"
   },
   "outputs": [],
   "source": [
    "test_df[2] = m\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.113627Z",
     "start_time": "2021-03-27T00:51:30.634Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "IybR8dA8N1I4"
   },
   "outputs": [],
   "source": [
    "test_df[0]=test_df[0].apply(lambda x:x[:-1])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.114506Z",
     "start_time": "2021-03-27T00:51:30.635Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "awVBkOVKPRNE"
   },
   "outputs": [],
   "source": [
    "sub_id=test_df[0].values\n",
    "pres_all = sub.tolist()\n",
    "str_w=''\n",
    "with open(f'{save_path}/submit_{oof_score}.csv','w') as f:\n",
    "    for i in range(len(sub_id)):\n",
    "        pres_fold = pres_all[i]\n",
    "        pres_fold=[str(p) for p in pres_fold]\n",
    "        prob = \" \".join(pres_fold)\n",
    "        str_w+=sub_id[i]+'|'+','+'|'+prob+'\\n'\n",
    "    str_w=str_w.strip('\\n')\n",
    "    f.write(str_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T00:55:55.115188Z",
     "start_time": "2021-03-27T00:51:30.636Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "mirMQlPPmHNI"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv(f\"{save_path}/oof_test_{score}.csv\")\n",
    "oof_train.to_csv(f\"{save_path}/oof_train_{score}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BMxuk94uvEhw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "torch_transformers_gpu_macbert_model-多标签分类v2.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
