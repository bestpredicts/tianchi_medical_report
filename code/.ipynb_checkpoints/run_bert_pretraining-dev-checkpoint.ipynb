{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:08.749452Z",
     "start_time": "2021-03-20T11:22:08.744799Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:08.756489Z",
     "start_time": "2021-03-20T11:22:08.753089Z"
    },
    "id": "AEjsYjfN_3si"
   },
   "outputs": [],
   "source": [
    "train_path = \"../input/track1_round1_train_20210222.csv\"\n",
    "test_path = \"../input/track1_round1_testA_20210222.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.080149Z",
     "start_time": "2021-03-20T11:22:08.758074Z"
    },
    "id": "Yo0TdSbbAH2O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.117432Z",
     "start_time": "2021-03-20T11:22:09.081507Z"
    },
    "id": "H2PtNUAxB1PQ"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path,sep=',',header=None)\n",
    "test_df = pd.read_csv(test_path,sep=',',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.147104Z",
     "start_time": "2021-03-20T11:22:09.118579Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 30575,
     "status": "ok",
     "timestamp": 1614608526974,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "eivMirH4B5Ei",
    "outputId": "99387b90-eb08-4e59-9dd9-9b55aed4fb0b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>|623 328 538 382 399 400 478 842 698 137 492 2...</td>\n",
       "      <td>|2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>|48 328 538 382 809 623 434 355 382 382 363 14...</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>|623 656 293 851 636 842 698 493 338 266 369 6...</td>\n",
       "      <td>|15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>|48 328 380 259 439 107 380 265 172 470 290 69...</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>|623 328 399 698 493 338 266 14 177 415 511 64...</td>\n",
       "      <td>|16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1     2\n",
       "0  0|  |623 328 538 382 399 400 478 842 698 137 492 2...   |2 \n",
       "1  1|  |48 328 538 382 809 623 434 355 382 382 363 14...     |\n",
       "2  2|  |623 656 293 851 636 842 698 493 338 266 369 6...  |15 \n",
       "3  3|  |48 328 380 259 439 107 380 265 172 470 290 69...     |\n",
       "4  4|  |623 328 399 698 493 338 266 14 177 415 511 64...  |16 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.161292Z",
     "start_time": "2021-03-20T11:22:09.148653Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 30569,
     "status": "ok",
     "timestamp": 1614608526975,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "vDTAh3DrCPwL",
    "outputId": "9f843319-df4c-409e-f70a-e4ad73d54a4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>623 328 538 382 399 400 478 842 698 137 492 26...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>48 328 538 382 809 623 434 355 382 382 363 145...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>623 656 293 851 636 842 698 493 338 266 369 69...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>48 328 380 259 439 107 380 265 172 470 290 693...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>623 328 399 698 493 338 266 14 177 415 511 647...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1    2\n",
       "0  0|  623 328 538 382 399 400 478 842 698 137 492 26...   2 \n",
       "1  1|  48 328 538 382 809 623 434 355 382 382 363 145...     \n",
       "2  2|  623 656 293 851 636 842 698 493 338 266 369 69...  15 \n",
       "3  3|  48 328 380 259 439 107 380 265 172 470 290 693...     \n",
       "4  4|  623 328 399 698 493 338 266 14 177 415 511 647...  16 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[1]=train_df[1].apply(lambda x:x[1:-1])\n",
    "train_df[2]=train_df[2].apply(lambda x:x[1:])\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.169242Z",
     "start_time": "2021-03-20T11:22:09.162431Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 30559,
     "status": "ok",
     "timestamp": 1614608526975,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "XYkg87_dFkM-",
    "outputId": "bc806889-f6e5-466d-c29b-117e31700a3f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>852 328 697 538 142 355 582 800 728 4 647 169 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>380 358 343 654 171 832 47 832 690 693 48 563 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>751 335 834 582 717 583 585 693 623 328 107 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>623 328 649 582 488 12 578 623 538 382 382 265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>83 293 398 797 382 363 145 424 693 698 800 691...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1\n",
       "0  0|  852 328 697 538 142 355 582 800 728 4 647 169 ...\n",
       "1  1|  380 358 343 654 171 832 47 832 690 693 48 563 ...\n",
       "2  2|  751 335 834 582 717 583 585 693 623 328 107 38...\n",
       "3  3|  623 328 649 582 488 12 578 623 538 382 382 265...\n",
       "4  4|  83 293 398 797 382 363 145 424 693 698 800 691..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[1] = test_df[1].apply(lambda x:x[1:-1])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.177868Z",
     "start_time": "2021-03-20T11:22:09.171155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0|</td>\n",
       "      <td>623 328 538 382 399 400 478 842 698 137 492 26...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1|</td>\n",
       "      <td>48 328 538 382 809 623 434 355 382 382 363 145...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2|</td>\n",
       "      <td>623 656 293 851 636 842 698 493 338 266 369 69...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3|</td>\n",
       "      <td>48 328 380 259 439 107 380 265 172 470 290 693...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4|</td>\n",
       "      <td>623 328 399 698 493 338 266 14 177 415 511 647...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1    2\n",
       "0  0|  623 328 538 382 399 400 478 842 698 137 492 26...   2 \n",
       "1  1|  48 328 538 382 809 623 434 355 382 382 363 145...     \n",
       "2  2|  623 656 293 851 636 842 698 493 338 266 369 69...  15 \n",
       "3  3|  48 328 380 259 439 107 380 265 172 470 290 693...     \n",
       "4  4|  623 328 399 698 493 338 266 14 177 415 511 647...  16 "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([train_df,test_df])\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.182712Z",
     "start_time": "2021-03-20T11:22:09.179673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n"
     ]
    }
   ],
   "source": [
    "print(len(all_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.643182Z",
     "start_time": "2021-03-20T11:22:09.184063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12935 65\n"
     ]
    }
   ],
   "source": [
    " from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_df,va_df =  train_test_split(all_df,test_size=0.005,shuffle=True)\n",
    "print(len(tr_df),len(va_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.824138Z",
     "start_time": "2021-03-20T11:22:09.644670Z"
    },
    "id": "e0oG5bxPba7w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "un_wr_coount =0\n",
    "\n",
    "with open(\"all_data.txt\",'w') as f:\n",
    "    for  l in range(len(all_df)):\n",
    "        text2 = all_df[1].values[l]\n",
    "        text = text2.split(\"693\")\n",
    "        t_len = len(text)\n",
    "        \n",
    "        if len(text2.split())<3:\n",
    "            continue\n",
    "        \n",
    "        if t_len == 1:\n",
    "            f.write(text2+\"\\n\")\n",
    "            f.write(text2+\"\\n\")\n",
    "            f.write(\"\"+\"\\n\")\n",
    "        else:\n",
    "            m = [i for i in range(t_len)]\n",
    "            random.shuffle(m)\n",
    "            m = m[0]\n",
    "            \n",
    "            first = \" 693 \".join(text[:m])\n",
    "            second = \" 693 \".join(text[m:])\n",
    "            if len(first)>1 and len(first.split())>1  and len(second)>1 and len(second.split())>1:\n",
    "                f.write(first+\"\\n\")\n",
    "                f.write(second+\"\\n\")\n",
    "                f.write(\"\"+\"\\n\")\n",
    "            elif len(text2)>1 and len(text2.split())>1:\n",
    "                f.write(text2+\"\\n\")\n",
    "                f.write(text2+\"\\n\")\n",
    "                f.write(\"\"+\"\\n\")\n",
    "            else:\n",
    "                un_wr_coount+=1\n",
    "\n",
    "print(un_wr_coount)\n",
    "        \n",
    "        \n",
    "\n",
    "with open(\"train_data.txt\",'w') as f:\n",
    "    for  l in range(len(tr_df)):\n",
    "\n",
    "        \n",
    "        text2 = tr_df[1].values[l]\n",
    "        f.write(text2+\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"valid_data.txt\",'w') as f:\n",
    "    for  l in range(len(va_df)):\n",
    "\n",
    "        text2 = va_df[1].values[l]\n",
    "        f.write(text2+\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:09.955321Z",
     "start_time": "2021-03-20T11:22:09.826147Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31881,
     "status": "ok",
     "timestamp": 1614608528305,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "D0owh0q_bw86",
    "outputId": "e0e8880e-b655-4d59-ae63-203361ae3d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623 328 538 382 399 400 478 842 698 137 492 266 521 177 415 381  693  700 132 706 317 534 830 290 512 729 327 548 520 445 51 240 711 818 445 358 240 711 \r\n",
      " 623 328 380 172 54 175 563 470 609 \r\n",
      "\r\n",
      "48 328 538 382 809 623 434 355 382 382 363 145 424 389  693  808 266 751 335 832 47 \r\n",
      " 583 328 305 206 461 204 48 328 740 204 411 204 549 728 832 122 \r\n",
      "\r\n",
      "623 656 293 851 636 842 698 493 338 266 369 691 693 380 136 363 399 556 698 66 432 449 177 830 381 332 290 380 26 343 28 177 415 832 14 \r\n",
      "623 656 293 851 636 842 698 493 338 266 369 691 693 380 136 363 399 556 698 66 432 449 177 830 381 332 290 380 26 343 28 177 415 832 14 \r\n",
      "\r\n",
      "48 328 380 259 439 107 380 265 172 470 290  693  556 698 54 623 34 138 351 761  693  657 305 342 809 618 282 300 654 556 698 432 449  693  380 834 809 343 809 832 47 \r\n",
      " 514 569 428 614 34 846 138  693  358 380 136 363 399 556 698 313 66 432 449 177 415 145  693  380 172 809 380 654 439 380 834 832 47 750 256 514 837 231 113 256 \r\n",
      "\r\n",
      "623 328 399 698 493 338 266 14 177 415 511 647 \r\n",
      " 852 60 328 380 172 54 788 591 487 \r\n",
      "\r\n",
      "80 328 328 54 172 439 741 380 172 842 698 177 777 415 832 14 381 \r\n",
      " 623 328 697 382 38 582 382 363 177 257 415 145 755 404 386 106 566 521 \r\n",
      "\r\n",
      "48 322 795 856 374 439 48 328 443 380 597 172 320 842 698 494 149 266 218 415 106 521 79 693 380 361 200 737 813 306 693 556 698 554 232 823 34 138 351 761 693 305 654 809 282 300 654 678 195 698 432 449 693 66 834 809 343 809 654 556 104 698 832 47 617 256 514 129 231 614 34 138 693 91 382 569 231 134 698 313 66 432 623 \r\n",
      "48 322 795 856 374 439 48 328 443 380 597 172 320 842 698 494 149 266 218 415 106 521 79 693 380 361 200 737 813 306 693 556 698 554 232 823 34 138 351 761 693 305 654 809 282 300 654 678 195 698 432 449 693 66 834 809 343 809 654 556 104 698 832 47 617 256 514 129 231 614 34 138 693 91 382 569 231 134 698 313 66 432 623 \r\n",
      "\r\n",
      "623 328 659 486 582 162 711 289 606 405 809 78 477  693  697 777 582 162 716 854 832 122 \r\n",
      " 697 582 38 582 2 498 165 397 455  693  724 328 697 698 494 504 382 672 514 381 \r\n",
      "\r\n",
      "852 328 471 585 117 458 399 607 \r\n",
      " 380 522 623 304 160 380 303 789 439 852 328 419 571 769 256 661 809 621 499 300 832 582 698 493 338 266 521 177 415 381 \r\n",
      "\r\n",
      "229 172 200 737 437 547 651 \r\n",
      " 623 328 355 653 382 579 488 776 591 487  693  91 400 478 698 477 300 797 415 381 \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -30 all_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:10.084587Z",
     "start_time": "2021-03-20T11:22:09.959559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39000 all_data.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l all_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:10.094072Z",
     "start_time": "2021-03-20T11:22:10.088703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n"
     ]
    }
   ],
   "source": [
    "print(len(all_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:10.307416Z",
     "start_time": "2021-03-20T11:22:10.096477Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33956,
     "status": "ok",
     "timestamp": 1614608530386,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "cm0o-5BkiT9W",
    "outputId": "3137d0b5-1a59-41c8-8a05-145951ccf4cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 105\n",
      "693 106\n",
      "698 107\n",
      "380 108\n",
      "415 109\n",
      "177 110\n",
      "381 111\n",
      "809 112\n",
      "623 113\n",
      "858\n",
      "963\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "    \n",
    "dic={}\n",
    "sep_list = ['[PAD]',\n",
    " '[unused1]',\n",
    " '[unused2]',\n",
    " '[unused3]',\n",
    " '[unused4]',\n",
    " '[unused5]',\n",
    " '[unused6]',\n",
    " '[unused7]',\n",
    " '[unused8]',\n",
    " '[unused9]',\n",
    " '[unused10]',\n",
    " '[unused11]',\n",
    " '[unused12]',\n",
    " '[unused13]',\n",
    " '[unused14]',\n",
    " '[unused15]',\n",
    " '[unused16]',\n",
    " '[unused17]',\n",
    " '[unused18]',\n",
    " '[unused19]',\n",
    " '[unused20]',\n",
    " '[unused21]',\n",
    " '[unused22]',\n",
    " '[unused23]',\n",
    " '[unused24]',\n",
    " '[unused25]',\n",
    " '[unused26]',\n",
    " '[unused27]',\n",
    " '[unused28]',\n",
    " '[unused29]',\n",
    " '[unused30]',\n",
    " '[unused31]',\n",
    " '[unused32]',\n",
    " '[unused33]',\n",
    " '[unused34]',\n",
    " '[unused35]',\n",
    " '[unused36]',\n",
    " '[unused37]',\n",
    " '[unused38]',\n",
    " '[unused39]',\n",
    " '[unused40]',\n",
    " '[unused41]',\n",
    " '[unused42]',\n",
    " '[unused43]',\n",
    " '[unused44]',\n",
    " '[unused45]',\n",
    " '[unused46]',\n",
    " '[unused47]',\n",
    " '[unused48]',\n",
    " '[unused49]',\n",
    " '[unused50]',\n",
    " '[unused51]',\n",
    " '[unused52]',\n",
    " '[unused53]',\n",
    " '[unused54]',\n",
    " '[unused55]',\n",
    " '[unused56]',\n",
    " '[unused57]',\n",
    " '[unused58]',\n",
    " '[unused59]',\n",
    " '[unused60]',\n",
    " '[unused61]',\n",
    " '[unused62]',\n",
    " '[unused63]',\n",
    " '[unused64]',\n",
    " '[unused65]',\n",
    " '[unused66]',\n",
    " '[unused67]',\n",
    " '[unused68]',\n",
    " '[unused69]',\n",
    " '[unused70]',\n",
    " '[unused71]',\n",
    " '[unused72]',\n",
    " '[unused73]',\n",
    " '[unused74]',\n",
    " '[unused75]',\n",
    " '[unused76]',\n",
    " '[unused77]',\n",
    " '[unused78]',\n",
    " '[unused79]',\n",
    " '[unused80]',\n",
    " '[unused81]',\n",
    " '[unused82]',\n",
    " '[unused83]',\n",
    " '[unused84]',\n",
    " '[unused85]',\n",
    " '[unused86]',\n",
    " '[unused87]',\n",
    " '[unused88]',\n",
    " '[unused89]',\n",
    " '[unused90]',\n",
    " '[unused91]',\n",
    " '[unused92]',\n",
    " '[unused93]',\n",
    " '[unused94]',\n",
    " '[unused95]',\n",
    " '[unused96]',\n",
    " '[unused97]',\n",
    " '[unused98]',\n",
    " '[unused99]',\n",
    " '[UNK]',\n",
    " '[CLS]',\n",
    " '[SEP]',\n",
    " '[MASK]',\n",
    " '<S>']\n",
    "\n",
    "\n",
    "for idx,token in enumerate(sep_list):\n",
    "     dic[token]= idx\n",
    "\n",
    "# #添加 保证词表大小和bert一样大 而已\n",
    "# for i in range(20706,21128):\n",
    "#     dic[str(i]=i\n",
    "# print(len(dic))\n",
    "\n",
    "\n",
    "conter=Counter()\n",
    "\n",
    "with open(\"all_data.txt\",'r') as f:\n",
    "    text = f.readlines()\n",
    "    for idx,txt in enumerate(text):\n",
    "        # if idx>100:\n",
    "        #     break\n",
    "        for char in txt.split():\n",
    "            conter[char]+=1\n",
    "\n",
    "most_common=conter.most_common(1000000) \n",
    "\n",
    "cont=0\n",
    "for idx,x in enumerate(most_common):\n",
    "    if x[1]>0:\n",
    "        dic[x[0]]=len(dic)\n",
    "    cont+=1\n",
    "    if cont<10:\n",
    "        print(x[0],dic[x[0]])\n",
    "print(cont)\n",
    "\n",
    "\n",
    "import json \n",
    "\n",
    "try:\n",
    "    os.mkdir(\"new_bert_v2\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dic = sorted(dic.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.343907Z",
     "start_time": "2021-03-20T11:22:10.308993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n",
      "963\n",
      "20165\n",
      "[PAD]\n",
      "[unused1]\n",
      "[unused2]\n",
      "[unused3]\n",
      "[unused4]\n",
      "[unused5]\n",
      "[unused6]\n",
      "[unused7]\n",
      "[unused8]\n",
      "[unused9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# BERT词频\n",
    "import json\n",
    "from transformers import BertTokenizer\n",
    "from bert4keras.tokenizers import load_vocab\n",
    "\n",
    "MODEL_PATH = '/home/root1/DY/uer_chinese_roberta_L-12_H-768'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(tokenizer.vocab))\n",
    "print(len(dic))\n",
    "diff = len(tokenizer.vocab)-len(dic)\n",
    "print(diff)\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"new_bert\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "with open(\"new_bert/new_vocab.txt\",'w') as f:\n",
    "    for idx,key in enumerate(dic):\n",
    "        f.write(key[0]+u\"\\n\")\n",
    "        if idx<10:\n",
    "            print(key[0])\n",
    "        \n",
    "    for i in range(422):\n",
    "        \n",
    "        f.write(\"100000_%s\\n\"%i)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.374693Z",
     "start_time": "2021-03-20T11:22:12.345687Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38327,
     "status": "ok",
     "timestamp": 1614608534763,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "wTot7YX41nm2",
    "outputId": "6ca8dc48-fd75-45e9-ff25-8f43559e8fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 330, 929, 166, 202, 261, 439, 549, 731, 711, 102, 166, 731, 664, 788, 313, 510, 202, 102]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"new_bert/\")\n",
    "\n",
    "\n",
    "text = \"[CLS] 17 18 12 19 20 21 22 23 24[SEP]12 23 25 6 26 27 19 [SEP]\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "print(tokenizer.encode_plus(text,add_special_tokens=False)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.386404Z",
     "start_time": "2021-03-20T11:22:12.375953Z"
    },
    "id": "GMDzH8ko1a9a"
   },
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# paths = ['demo_data.txt']#[str(x) for x in Path(\".\").glob(\"*.txt\")]\n",
    "\n",
    "# # Initialize a tokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# # Customize training\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.390229Z",
     "start_time": "2021-03-20T11:22:12.387855Z"
    },
    "id": "T2QA2SvyFqLp"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     os.mkdir(\"roberta_pretrain\")\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.394316Z",
     "start_time": "2021-03-20T11:22:12.392116Z"
    },
    "id": "bALTjOesEbU1"
   },
   "outputs": [],
   "source": [
    "# tokenizer.save_model(\"roberta_pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.398370Z",
     "start_time": "2021-03-20T11:22:12.396115Z"
    },
    "id": "YZMVZVrXWz2W"
   },
   "outputs": [],
   "source": [
    "# from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "# from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer(\n",
    "#     'roberta_pretrain/vocab.json',\n",
    "#     'roberta_pretrain/merges.txt',\n",
    "# )\n",
    "\n",
    "\n",
    "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "# )\n",
    "# tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "\n",
    "# tokenizer.encode(\"623 328 538 382 399 400 478 842 698 137 492 266 521 177 415 381 693 700 132 706 317 534 830 290 512 729 327 548 520 445 51 240 711 818 445 358 240 711 693 623 328 \").tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.417374Z",
     "start_time": "2021-03-20T11:22:12.399625Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38310,
     "status": "ok",
     "timestamp": 1614608534766,
     "user": {
      "displayName": "yong deng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSndyur9-PDshBAHN6N5k7zPMUA9ZJNWM3SXlv=s64",
      "userId": "04661099788502381780"
     },
     "user_tz": -480
    },
    "id": "8IHZjWrLhKn1",
    "outputId": "9402e2c8-6195-46cd-8720-23646f0a3ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "vocab_size = len(dic)+diff\n",
    "print(vocab_size)\n",
    "config = BertConfig.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.423966Z",
     "start_time": "2021-03-20T11:22:12.421517Z"
    }
   },
   "outputs": [],
   "source": [
    "# a\\n\n",
    "# b\\n\n",
    "# \"\"\\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T11:22:12.507255Z",
     "start_time": "2021-03-20T11:22:12.427211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 07:22:12 - __main__ - INFO: - this is info message\n",
      "/home/root1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:60: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "2021-03-20 07:22:12 - __main__ - WARNING: - this is warn message\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"BERT finetuning runner.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import random\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pytorch_transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_transformers.modeling_bert import BertForPreTraining\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "\n",
    "import logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)  # 不加名称设置root logger\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s: - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# 使用FileHandler输出到文件\n",
    "fh = logging.FileHandler('ngram_bert_pretrain_loss.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# 使用StreamHandler输出到屏幕\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# 添加两个Handler\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "logger.info('this is info message')\n",
    "logger.warn('this is warn message')\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"new_bert/new_vocab.txt\", do_lower_case=True)\n",
    "\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path, tokenizer, seq_len, encoding=\"utf-8\", corpus_lines=None, on_memory=True):\n",
    "        self.vocab = tokenizer.vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.on_memory = on_memory\n",
    "        self.corpus_lines = corpus_lines  # number of non-empty lines in input corpus\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "        self.current_doc = 0  # to avoid random sentence from same doc\n",
    "\n",
    "        # for loading samples directly from file\n",
    "        self.sample_counter = 0  # used to keep track of full epochs on file\n",
    "        self.line_buffer = None  # keep second sentence of a pair in memory and use as first sentence in next pair\n",
    "\n",
    "        # for loading samples in memory\n",
    "        self.current_random_doc = 0\n",
    "        self.num_docs = 0\n",
    "        self.sample_to_doc = [] # map sample index to doc and line\n",
    "\n",
    "        # load samples into memory\n",
    "        if on_memory:\n",
    "            self.all_docs = []\n",
    "            doc = []\n",
    "            self.corpus_lines = 0\n",
    "            with open(corpus_path, \"r\", encoding=encoding) as f:\n",
    "                for line in tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "                    line = line.strip()\n",
    "                    if line == \"\":\n",
    "                        if len(doc)>0: #bug\n",
    "                            self.all_docs.append(doc)\n",
    "                        doc = []\n",
    "                        #remove last added sample because there won't be a subsequent line anymore in the doc\n",
    "                        self.sample_to_doc.pop()\n",
    "                    elif len(line)>1 and len(line.split())>1:\n",
    "                        #store as one sample\n",
    "                        sample = {\"doc_id\": len(self.all_docs),\n",
    "                                  \"line\": len(doc)}\n",
    "                        self.sample_to_doc.append(sample)\n",
    "                        doc.append(line)\n",
    "                        self.corpus_lines = self.corpus_lines + 1\n",
    "\n",
    "            # if last row in file is not empty\n",
    "            if self.all_docs[-1] != doc:\n",
    "                self.all_docs.append(doc)\n",
    "                self.sample_to_doc.pop()\n",
    "\n",
    "            self.num_docs = len(self.all_docs)\n",
    "\n",
    "        # load samples later lazily from disk\n",
    "        else:\n",
    "            if self.corpus_lines is None:\n",
    "                with open(corpus_path, \"r\", encoding=encoding) as f:\n",
    "                    self.corpus_lines = 0\n",
    "                    for line in tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "                        if line.strip() == \"\":\n",
    "                            self.num_docs += 1\n",
    "                        else:\n",
    "                            self.corpus_lines += 1\n",
    "\n",
    "                    # if doc does not end with empty line\n",
    "                    if line.strip() != \"\":\n",
    "                        self.num_docs += 1\n",
    "\n",
    "            self.file = open(corpus_path, \"r\", encoding=encoding)\n",
    "            self.random_file = open(corpus_path, \"r\", encoding=encoding)\n",
    "            \n",
    "            \n",
    "        print(\"init doc size\",len(self.all_docs))\n",
    "\n",
    "    def __len__(self):\n",
    "        # last line of doc won't be used, because there's no \"nextSentence\". Additionally, we start counting at 0.\n",
    "        return self.corpus_lines - self.num_docs - 1\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        cur_id = self.sample_counter\n",
    "        self.sample_counter += 1\n",
    "        if not self.on_memory:\n",
    "            # after one epoch we start again from beginning of file\n",
    "            if cur_id != 0 and (cur_id % len(self) == 0):\n",
    "                self.file.close()\n",
    "                self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "\n",
    "        t1, t2, is_next_label = self.random_sent(item)\n",
    "\n",
    "        # tokenize\n",
    "        tokens_a = self.tokenizer.tokenize(t1)\n",
    "        tokens_b = self.tokenizer.tokenize(t2)\n",
    "\n",
    "        # combine to one sample\n",
    "        cur_example = InputExample(guid=cur_id, tokens_a=tokens_a, tokens_b=tokens_b, is_next=is_next_label)\n",
    "\n",
    "        # transform sample to features\n",
    "        cur_features = convert_example_to_features(cur_example, self.seq_len, self.tokenizer)\n",
    "\n",
    "        cur_tensors = (torch.tensor(cur_features.input_ids),\n",
    "                       torch.tensor(cur_features.input_mask),\n",
    "                       torch.tensor(cur_features.segment_ids),\n",
    "                       torch.tensor(cur_features.lm_label_ids),\n",
    "                       torch.tensor(cur_features.is_next))\n",
    "\n",
    "        return cur_tensors\n",
    "\n",
    "    def random_sent(self, index):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of two sentences. With prob. 50% these are two subsequent sentences\n",
    "        from one doc. With 50% the second sentence will be a random one from another doc.\n",
    "        :param index: int, index of sample.\n",
    "        :return: (str, str, int), sentence 1, sentence 2, isNextSentence Label\n",
    "        \"\"\"\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "        if random.random() > 0.5:\n",
    "            label = 0\n",
    "        else:\n",
    "            t2 = self.get_random_line()\n",
    "            label = 1\n",
    "\n",
    "        assert len(t1) > 0\n",
    "        assert len(t2) > 0\n",
    "        return t1, t2, label\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of a pair of two subsequent lines from the same doc.\n",
    "        :param item: int, index of sample.\n",
    "        :return: (str, str), two subsequent sentences from corpus\n",
    "        \"\"\"\n",
    "        t1 = \"\"\n",
    "        t2 = \"\"\n",
    "        assert item < self.corpus_lines\n",
    "        if self.on_memory:\n",
    "            sample = self.sample_to_doc[item]\n",
    "            t1 = self.all_docs[sample[\"doc_id\"]][sample[\"line\"]]\n",
    "            t2 = self.all_docs[sample[\"doc_id\"]][sample[\"line\"]+1]\n",
    "            # used later to avoid random nextSentence from same doc\n",
    "            self.current_doc = sample[\"doc_id\"]\n",
    "            return t1, t2\n",
    "        else:\n",
    "            if self.line_buffer is None:\n",
    "                # read first non-empty line of file\n",
    "                while t1 == \"\" :\n",
    "                    t1 = next(self.file).strip()\n",
    "                    t2 = next(self.file).strip()\n",
    "            else:\n",
    "                # use t2 from previous iteration as new t1\n",
    "                t1 = self.line_buffer\n",
    "                t2 = next(self.file).strip()\n",
    "                # skip empty rows that are used for separating documents and keep track of current doc id\n",
    "                while t2 == \"\" or t1 == \"\":\n",
    "                    t1 = next(self.file).strip()\n",
    "                    t2 = next(self.file).strip()\n",
    "                    self.current_doc = self.current_doc+1\n",
    "            self.line_buffer = t2\n",
    "\n",
    "        assert t1 != \"\"\n",
    "        assert t2 != \"\"\n",
    "        return t1, t2\n",
    "\n",
    "    def get_random_line(self):\n",
    "        \"\"\"\n",
    "        Get random line from another document for nextSentence task.\n",
    "        :return: str, content of one line\n",
    "        \"\"\"\n",
    "        # Similar to original tf repo: This outer loop should rarely go for more than one iteration for large\n",
    "        # corpora. However, just to be careful, we try to make sure that\n",
    "        # the random document is not the same as the document we're processing.\n",
    "        for _ in range(10):\n",
    "            if self.on_memory:\n",
    "#                 print(len(self.all_docs))\n",
    "                rand_doc_idx = random.randint(0+2, len(self.all_docs)-10)\n",
    "                rand_doc = self.all_docs[rand_doc_idx]\n",
    "                try:\n",
    "                    line = rand_doc[random.randrange(len(rand_doc))]\n",
    "                except:\n",
    "                    print(\"ERROR\",len(rand_doc))\n",
    "                    print(rand_doc_idx)\n",
    "                    print(self.all_docs[rand_doc_idx])\n",
    "                    print(self.all_docs[1])\n",
    "\n",
    "                    line = rand_doc[random.randrange(len(rand_doc))]\n",
    "            else:\n",
    "                rand_index = random.randint(1, self.corpus_lines if self.corpus_lines < 1000 else 1000)\n",
    "                #pick random line\n",
    "                for _ in range(rand_index):\n",
    "                    line = self.get_next_line()\n",
    "            #check if our picked random line is really from another doc like we want it to be\n",
    "            if self.current_random_doc != self.current_doc:\n",
    "                break\n",
    "        return line\n",
    "\n",
    "    def get_next_line(self):\n",
    "        \"\"\" Gets next line of random_file and starts over when reaching end of file\"\"\"\n",
    "        try:\n",
    "            line = next(self.random_file).strip()\n",
    "            #keep track of which document we are currently looking at to later avoid having the same doc as t1\n",
    "            if line == \"\":\n",
    "                self.current_random_doc = self.current_random_doc + 1\n",
    "                line = next(self.random_file).strip()\n",
    "        except StopIteration:\n",
    "            self.random_file.close()\n",
    "            self.random_file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "            line = next(self.random_file).strip()\n",
    "        return line\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for the language model.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, tokens_a, tokens_b=None, is_next=None, lm_labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            tokens_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            tokens_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.tokens_a = tokens_a\n",
    "        self.tokens_b = tokens_b\n",
    "        self.is_next = is_next  # nextSentence\n",
    "        self.lm_labels = lm_labels  # masked words for language model\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, is_next, lm_label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_next = is_next\n",
    "        self.lm_label_ids = lm_label_ids\n",
    "\n",
    "\n",
    "\n",
    "def random_word(tokens, tokenizer):\n",
    "    \"\"\"\n",
    "    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n",
    "    :param tokens: list of str, tokenized sentence.\n",
    "    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\n",
    "    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n",
    "    \"\"\"\n",
    "    output_label = [-1 for i  in range(len(tokens))] # -1 \n",
    "    org_token = tokens.copy()\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        prob = random.random()\n",
    "        # mask token with 15% probability\n",
    "        if prob < 0.15:\n",
    "            prob /= 0.15\n",
    "\n",
    "            # 80% randomly change token to mask token  by dengyong  Fri Mar 19 09:50:13 CST 2021\n",
    "            if prob < 0.8:\n",
    "                prob_2 = random.random()  \n",
    "                if prob_2<0.3:#   0.3进行 2-gram mask\n",
    "                    span_len=2\n",
    "                    try:\n",
    "                        for j in range(span_len):\n",
    "                            tokens[i+j]= \"[MASK]\"\n",
    "                            output_label[i+j] = tokenizer.vocab[org_token[i+j]]\n",
    "                    except:\n",
    "                        tokens[i] = \"[MASK]\"\n",
    "                        output_label[i] = tokenizer.vocab[org_token[i]]\n",
    "                        \n",
    "                        \n",
    "                elif prob_2<0.6:#   0.3进行 3-gram mask\n",
    "                    span_len = 3\n",
    "                    try:\n",
    "                        for j in range(span_len):\n",
    "                            tokens[i+j]= \"[MASK]\"\n",
    "                            output_label[i+j] = tokenizer.vocab[org_token[i+j]]\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        tokens[i] = \"[MASK]\"\n",
    "                        output_label[i] = tokenizer.vocab[org_token[i]]\n",
    "                        \n",
    "                    \n",
    "                        \n",
    "                else: # 0.4 1-gram mask\n",
    "                    tokens[i] = \"[MASK]\"\n",
    "                    output_label[i] = tokenizer.vocab[org_token[i]]\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "            # 10% randomly change token to random token\n",
    "            elif prob < 0.9:\n",
    "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\n",
    "\n",
    "            # -> rest 10% randomly keep current token\n",
    "\n",
    "            # append current token to output (we will predict these later)\n",
    "#             try:\n",
    "#                 output_label.append(tokenizer.vocab[token])\n",
    "#             except KeyError:\n",
    "#                 # For unknown words (should not occur with BPE vocab)\n",
    "#                 output_label.append(tokenizer.vocab[\"[UNK]\"])\n",
    "#                 logger.warning(\"Cannot find token '{}' in vocab. Using [UNK] insetad\".format(token))\n",
    "#         else:\n",
    "#             # no masking token (will be ignored by loss function later)\n",
    "#             output_label.append(-1)\n",
    "\n",
    "    return tokens, output_label\n",
    "\n",
    "\n",
    "def convert_example_to_features(example, max_seq_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with\n",
    "    IDs, LM labels, input_mask, CLS and SEP tokens etc.\n",
    "    :param example: InputExample, containing sentence input as strings and is_next label\n",
    "    :param max_seq_length: int, maximum length of sequence.\n",
    "    :param tokenizer: Tokenizer\n",
    "    :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training)\n",
    "    \"\"\"\n",
    "    tokens_a = example.tokens_a\n",
    "    tokens_b = example.tokens_b\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "    tokens_a, t1_label = random_word(tokens_a, tokenizer)\n",
    "    tokens_b, t2_label = random_word(tokens_b, tokenizer)\n",
    "    # concatenate lm labels and account for CLS, SEP, SEP\n",
    "    lm_label_ids = ([-1] + t1_label + [-1] + t2_label + [-1])\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0   0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    assert len(tokens_b) > 0\n",
    "    for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        lm_label_ids.append(-1)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(lm_label_ids) == max_seq_length\n",
    "\n",
    "    if example.guid < 5:\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"guid: %s\" % (example.guid))\n",
    "        logger.info(\"tokens: %s\" % \" \".join(\n",
    "                [str(x) for x in tokens]))\n",
    "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        logger.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        logger.info(\"LM label: %s \" % (lm_label_ids))\n",
    "        logger.info(\"Is next sentence label: %s \" % (example.is_next))\n",
    "\n",
    "    features = InputFeatures(input_ids=input_ids,\n",
    "                             input_mask=input_mask,\n",
    "                             segment_ids=segment_ids,\n",
    "                             lm_label_ids=lm_label_ids,\n",
    "                             is_next=example.is_next)\n",
    "    return features\n",
    "\n",
    "\n",
    "def main(args):\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     ## Required parameters\n",
    "#     parser.add_argument(\"--train_corpus\",\n",
    "#                         default=None,\n",
    "#                         type=str,\n",
    "#                         required=True,\n",
    "#                         help=\"The input train corpus.\")\n",
    "#     parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "#                         help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "#                              \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "#     parser.add_argument(\"--output_dir\",\n",
    "#                         default=None,\n",
    "#                         type=str,\n",
    "#                         required=True,\n",
    "#                         help=\"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "#     ## Other parameters\n",
    "#     parser.add_argument(\"--max_seq_length\",\n",
    "#                         default=128,\n",
    "#                         type=int,\n",
    "#                         help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "#                              \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "#                              \"than this will be padded.\")\n",
    "#     parser.add_argument(\"--do_train\",\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether to run training.\")\n",
    "#     parser.add_argument(\"--train_batch_size\",\n",
    "#                         default=32,\n",
    "#                         type=int,\n",
    "#                         help=\"Total batch size for training.\")\n",
    "#     parser.add_argument(\"--learning_rate\",\n",
    "#                         default=3e-5,\n",
    "#                         type=float,\n",
    "#                         help=\"The initial learning rate for Adam.\")\n",
    "#     parser.add_argument(\"--adam_epsilon\", \n",
    "#                         default=1e-8, \n",
    "#                         type=float,\n",
    "#                         help=\"Epsilon for Adam optimizer.\")\n",
    "#     parser.add_argument(\"--num_train_epochs\",\n",
    "#                         default=3.0,\n",
    "#                         type=float,\n",
    "#                         help=\"Total number of training epochs to perform.\")\n",
    "#     parser.add_argument(\"--warmup_steps\", \n",
    "#                         default=0, \n",
    "#                         type=int,\n",
    "#                         help=\"Linear warmup over warmup_steps.\")\n",
    "#     parser.add_argument(\"--no_cuda\",\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether not to use CUDA when available\")\n",
    "#     parser.add_argument(\"--on_memory\",\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether to load train samples into memory or use disk\")\n",
    "#     parser.add_argument(\"--do_lower_case\",\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
    "#     parser.add_argument(\"--local_rank\",\n",
    "#                         type=int,\n",
    "#                         default=-1,\n",
    "#                         help=\"local_rank for distributed training on gpus\")\n",
    "#     parser.add_argument('--seed',\n",
    "#                         type=int,\n",
    "#                         default=42,\n",
    "#                         help=\"random seed for initialization\")\n",
    "#     parser.add_argument('--gradient_accumulation_steps',\n",
    "#                         type=int,\n",
    "#                         default=1,\n",
    "#                         help=\"Number of updates steps to accumualte before performing a backward/update pass.\")\n",
    "#     parser.add_argument('--fp16',\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "#     parser.add_argument('--loss_scale',\n",
    "#                         type = float, default = 0,\n",
    "#                         help = \"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "#                         \"0 (default value): dynamic loss scaling.\\n\"\n",
    "#                         \"Positive power of 2: static loss scaling value.\\n\")\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if not args.do_train:\n",
    "        raise ValueError(\"Training is currently the only implemented execution option. Please set `do_train`.\")\n",
    "\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "    if not os.path.exists(args.output_dir) and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "#     tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    #train_examples = None\n",
    "    num_train_optimization_steps = None\n",
    "    if args.do_train:\n",
    "        print(\"Loading Train Dataset\", args.train_corpus)\n",
    "        train_dataset = BERTDataset(args.train_corpus, tokenizer, seq_len=args.max_seq_length,\n",
    "                                    corpus_lines=None, on_memory=args.on_memory)\n",
    "        num_train_optimization_steps = int(\n",
    "            len(train_dataset) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "        if args.local_rank != -1:\n",
    "            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "    # Prepare model\n",
    "    model = BertForPreTraining.from_pretrained(args.bert_model)\n",
    "    model.bert.word_embedding=torch.nn.Embedding(len(tokenizer.vocab),768) #随机初始化token embeeding\n",
    "    \n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    if args.local_rank != -1:\n",
    "        try:\n",
    "            from apex.parallel import DistributedDataParallel as DDP\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "        model = DDP(model)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    if args.do_train:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "\n",
    "        if args.fp16:\n",
    "            try:\n",
    "                from apex.optimizers import FP16_Optimizer\n",
    "                from apex.optimizers import FusedAdam\n",
    "            except ImportError:\n",
    "                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "            optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                                  lr=args.learning_rate,\n",
    "                                  bias_correction=False,\n",
    "                                  max_grad_norm=1.0)\n",
    "            if args.loss_scale == 0:\n",
    "                optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "            else:\n",
    "                optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n",
    "\n",
    "        else:\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=int(num_train_optimization_steps*0.1), t_total=num_train_optimization_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    if args.do_train:\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "\n",
    "        if args.local_rank == -1:\n",
    "            train_sampler = RandomSampler(train_dataset)\n",
    "        else:\n",
    "            #TODO: check if this works with current data generator from disk that relies on next(file)\n",
    "            # (it doesn't return item back by index)\n",
    "            train_sampler = DistributedSampler(train_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "        model.train()\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "#             tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, lm_label_ids, is_next = batch\n",
    "                outputs = model(input_ids, segment_ids, input_mask, lm_label_ids, is_next)\n",
    "                loss = outputs[0]\n",
    "                if n_gpu > 1:\n",
    "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "                if args.fp16:\n",
    "                    optimizer.backward(loss)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    \n",
    "    \n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    print(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    print_info = \"loss is :%f.  global_step:%d\"%((tr_loss - logging_loss) / args.logging_steps,global_step)\n",
    "                    logger.info(print_info)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "        # Save a trained model\n",
    "        if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "            logger.info(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(args.output_dir)\n",
    "            tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "\n",
    "class args:\n",
    "    train_corpus = 'all_data.txt'\n",
    "    bert_model = \"/home/root1/DY/uer_chinese_roberta_L-12_H-768\"\n",
    "    output_dir = \"n_gram_mask_bert_v1\"\n",
    "    max_seq_length = 60\n",
    "    do_train = True\n",
    "    train_batch_size = 200\n",
    "    learning_rate = 3e-5\n",
    "    adam_epsilon = 1e-8\n",
    "    num_train_epochs = 200\n",
    "    warmup_steps = 0\n",
    "    no_cuda = False\n",
    "    on_memory=True\n",
    "    do_lower_case=True\n",
    "    local_rank=-1\n",
    "    seed=2021\n",
    "    gradient_accumulation_steps=1\n",
    "    fp16=False\n",
    "    loss_scale=0\n",
    "    logging_steps = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-20T11:22:08.724Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 07:22:12 - __main__ - INFO: - device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "Loading Dataset: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Dataset all_data.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 39000it [00:00, 232887.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init doc size 13001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 07:22:19 - __main__ - INFO: - ***** Running training *****\n",
      "2021-03-20 07:22:19 - __main__ - INFO: -   Num examples = 12998\n",
      "2021-03-20 07:22:19 - __main__ - INFO: -   Batch size = 200\n",
      "2021-03-20 07:22:19 - __main__ - INFO: -   Num steps = 12800\n",
      "Epoch:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/65 [00:00<?, ?it/s]\u001b[A2021-03-20 07:22:19 - __main__ - INFO: - *** Example ***\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - guid: 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - tokens: [CLS] 852 328 431 582 38 582 [MASK] [MASK] 478 698 722 445 208 337 [MASK] 52 809 437 14 809 437 521 177 415 381 [MASK] [MASK] 708 [MASK] [MASK] [MASK] 218 [MASK] [MASK] [SEP] 596 502 700 698 494 504 493 [MASK] [MASK] 726 369 691 381 693 91 382 569 231 556 698 269 [SEP]\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_ids: 101 118 105 447 126 198 126 103 103 160 107 440 232 838 596 103 338 112 163 115 112 163 123 110 109 111 103 103 960 103 103 103 723 103 103 102 309 245 146 107 172 155 168 103 103 846 252 217 111 106 148 122 131 136 135 107 784 102 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - LM label: [-1, -1, -1, -1, -1, -1, -1, 486, 191, -1, -1, -1, -1, -1, -1, 151, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 127, 256, -1, 420, 639, 106, -1, 107, 192, -1, -1, -1, -1, -1, -1, -1, -1, 501, 114, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - Is next sentence label: 0 \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - *** Example ***\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - guid: 1\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - tokens: [CLS] 328 165 380 [MASK] 834 320 118 [MASK] [MASK] [MASK] [MASK] [MASK] 149 [MASK] 573 375 [MASK] [MASK] [MASK] 381 [SEP] [MASK] [MASK] [MASK] [MASK] 382 439 411 382 252 494 504 382 363 145 [MASK] [MASK] [MASK] [SEP]\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_ids: 101 105 672 108 103 132 185 483 103 103 103 103 103 149 103 747 475 103 103 103 111 102 103 103 103 103 122 127 196 122 725 172 155 122 137 171 103 103 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - LM label: [-1, -1, -1, -1, 694, -1, -1, -1, 137, 119, 107, 223, 155, -1, 272, -1, -1, 123, 110, 109, -1, -1, 134, 122, 915, 259, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 228, 234, 397, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - Is next sentence label: 1 \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - *** Example ***\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - guid: 2\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - tokens: [CLS] [MASK] 456 107 380 809 [MASK] [MASK] [MASK] [MASK] [MASK] 355 661 698 [MASK] 504 493 [MASK] 266 521 177 415 502 693 700 132 395 176 [SEP] 852 544 697 222 698 493 [MASK] [MASK] 177 548 386 521 447 [SEP]\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_ids: 101 103 665 294 108 112 103 103 103 103 103 138 133 107 103 155 168 103 114 123 110 109 245 106 146 176 219 222 102 118 634 134 947 107 168 103 103 110 200 262 123 498 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - LM label: [-1, 124, -1, -1, -1, -1, 317, 174, 478, 127, 134, -1, -1, -1, 230, -1, -1, 128, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 128, 114, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - Is next sentence label: 1 \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - *** Example ***\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - guid: 3\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - tokens: [CLS] 623 328 380 172 54 823 487 244 693 256 514 50 335 [MASK] [MASK] 256 [SEP] 380 200 556 698 313 66 432 449 693 256 514 569 70 231 614 387 [SEP]\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_ids: 101 113 105 108 116 175 178 173 712 106 117 129 251 220 103 103 117 102 108 182 135 107 153 140 145 144 106 117 129 131 675 136 195 699 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - LM label: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 322, 158, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - Is next sentence label: 0 \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - *** Example ***\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - guid: 4\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - tokens: [CLS] 380 172 100000_130 809 566 809 556 698 313 66 446 [MASK] 693 328 [MASK] [MASK] 54 823 809 34 138 [SEP] 229 [MASK] [MASK] 382 556 698 [MASK] [MASK] 449 [SEP]\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_ids: 101 108 116 1093 112 570 112 135 107 153 140 278 103 106 105 103 103 175 178 112 152 157 102 261 103 103 122 135 107 103 103 144 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2021-03-20 07:22:19 - __main__ - INFO: - LM label: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 144, -1, -1, 108, 116, -1, -1, -1, -1, -1, -1, -1, 179, 148, -1, -1, -1, 417, 254, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] \n",
      "2021-03-20 07:22:19 - __main__ - INFO: - Is next sentence label: 1 \n",
      "/home/root1/anaconda3/lib/python3.7/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "\n",
      "Iteration:   2%|▏         | 1/65 [00:00<00:39,  1.62it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 2/65 [00:01<00:36,  1.71it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 3/65 [00:01<00:35,  1.74it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 4/65 [00:02<00:34,  1.75it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 5/65 [00:02<00:34,  1.76it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 6/65 [00:03<00:33,  1.76it/s]\u001b[A\n",
      "Iteration:  11%|█         | 7/65 [00:03<00:32,  1.77it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 8/65 [00:04<00:32,  1.78it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 9/65 [00:05<00:31,  1.78it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 10/65 [00:05<00:30,  1.78it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 11/65 [00:06<00:30,  1.78it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 12/65 [00:06<00:29,  1.78it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 13/65 [00:07<00:29,  1.78it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 14/65 [00:07<00:28,  1.78it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 15/65 [00:08<00:28,  1.78it/s]\u001b[A\n",
      "Iteration:  25%|██▍       | 16/65 [00:09<00:27,  1.79it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 17/65 [00:09<00:26,  1.78it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 18/65 [00:10<00:26,  1.77it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 19/65 [00:10<00:25,  1.78it/s]\u001b[A\n",
      "Iteration:  31%|███       | 20/65 [00:11<00:25,  1.78it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 21/65 [00:11<00:24,  1.78it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 22/65 [00:12<00:24,  1.79it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 23/65 [00:12<00:23,  1.78it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 24/65 [00:13<00:22,  1.78it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 25/65 [00:14<00:22,  1.78it/s]\u001b[A\n",
      "Iteration:  40%|████      | 26/65 [00:14<00:21,  1.79it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 27/65 [00:15<00:21,  1.78it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 28/65 [00:15<00:20,  1.77it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 29/65 [00:16<00:20,  1.78it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 30/65 [00:16<00:19,  1.79it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 31/65 [00:17<00:18,  1.79it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 32/65 [00:18<00:18,  1.79it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 33/65 [00:18<00:17,  1.78it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 34/65 [00:19<00:17,  1.78it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 35/65 [00:19<00:16,  1.78it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 36/65 [00:20<00:16,  1.79it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 37/65 [00:20<00:15,  1.79it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 38/65 [00:21<00:15,  1.78it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 39/65 [00:21<00:14,  1.78it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 40/65 [00:22<00:14,  1.77it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 41/65 [00:23<00:13,  1.77it/s]\u001b[A\n",
      "Iteration:  65%|██████▍   | 42/65 [00:23<00:12,  1.78it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 43/65 [00:24<00:12,  1.78it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 44/65 [00:24<00:11,  1.78it/s]\u001b[A2021-03-20 07:22:44 - __main__ - INFO: - loss is :8.350803.  global_step:45\n",
      "\n",
      "Iteration:  69%|██████▉   | 45/65 [00:25<00:11,  1.78it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 8.350803014967177 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  71%|███████   | 46/65 [00:25<00:10,  1.78it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 47/65 [00:26<00:10,  1.78it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 48/65 [00:27<00:09,  1.78it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 49/65 [00:27<00:08,  1.78it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 50/65 [00:28<00:08,  1.78it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 51/65 [00:28<00:07,  1.77it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 52/65 [00:29<00:07,  1.78it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 53/65 [00:29<00:06,  1.77it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 54/65 [00:30<00:06,  1.77it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 55/65 [00:30<00:05,  1.78it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 56/65 [00:31<00:05,  1.77it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 57/65 [00:32<00:04,  1.78it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 58/65 [00:32<00:03,  1.78it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 59/65 [00:33<00:03,  1.79it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 60/65 [00:33<00:02,  1.78it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 61/65 [00:34<00:02,  1.78it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 62/65 [00:34<00:01,  1.78it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 63/65 [00:35<00:01,  1.78it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 64/65 [00:36<00:00,  1.78it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 65/65 [00:36<00:00,  1.78it/s]\u001b[A\n",
      "Epoch:   0%|          | 1/200 [00:36<2:01:17, 36.57s/it]\n",
      "Iteration:   0%|          | 0/65 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 1/65 [00:00<00:35,  1.80it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 2/65 [00:01<00:35,  1.78it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 3/65 [00:01<00:34,  1.78it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 4/65 [00:02<00:34,  1.77it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 5/65 [00:02<00:33,  1.78it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 6/65 [00:03<00:33,  1.78it/s]\u001b[A\n",
      "Iteration:  11%|█         | 7/65 [00:03<00:32,  1.79it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 8/65 [00:04<00:31,  1.79it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 9/65 [00:05<00:31,  1.79it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 10/65 [00:05<00:30,  1.79it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 11/65 [00:06<00:30,  1.79it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 12/65 [00:06<00:29,  1.78it/s]\u001b[A\n",
      "Iteration:  20%|██        | 13/65 [00:07<00:29,  1.78it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 14/65 [00:07<00:28,  1.79it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 15/65 [00:08<00:28,  1.79it/s]\u001b[A\n",
      "Iteration:  25%|██▍       | 16/65 [00:08<00:27,  1.79it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 17/65 [00:09<00:26,  1.78it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 18/65 [00:10<00:26,  1.78it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 19/65 [00:10<00:25,  1.78it/s]\u001b[A\n",
      "Iteration:  31%|███       | 20/65 [00:11<00:25,  1.78it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 21/65 [00:11<00:24,  1.78it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 22/65 [00:12<00:24,  1.78it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 23/65 [00:12<00:23,  1.79it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 24/65 [00:13<00:23,  1.77it/s]\u001b[A2021-03-20 07:23:09 - __main__ - INFO: - loss is :7.575076.  global_step:90\n",
      "\n",
      "Iteration:  38%|███▊      | 25/65 [00:14<00:22,  1.77it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 7.5750763999091255 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  40%|████      | 26/65 [00:14<00:21,  1.78it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 27/65 [00:15<00:21,  1.78it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 28/65 [00:15<00:20,  1.78it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 29/65 [00:16<00:20,  1.78it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 30/65 [00:16<00:19,  1.78it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 31/65 [00:17<00:19,  1.78it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 32/65 [00:17<00:18,  1.77it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 33/65 [00:18<00:18,  1.78it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 34/65 [00:19<00:17,  1.78it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 35/65 [00:19<00:16,  1.78it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 36/65 [00:20<00:16,  1.77it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 37/65 [00:20<00:15,  1.77it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 38/65 [00:21<00:15,  1.77it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 39/65 [00:21<00:14,  1.77it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 40/65 [00:22<00:14,  1.77it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 41/65 [00:23<00:13,  1.77it/s]\u001b[A\n",
      "Iteration:  65%|██████▍   | 42/65 [00:23<00:13,  1.77it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 43/65 [00:24<00:12,  1.77it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 44/65 [00:24<00:11,  1.77it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 45/65 [00:25<00:11,  1.77it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 46/65 [00:25<00:10,  1.77it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 47/65 [00:26<00:10,  1.77it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 48/65 [00:26<00:09,  1.77it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 49/65 [00:27<00:09,  1.77it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 50/65 [00:28<00:08,  1.77it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 51/65 [00:28<00:07,  1.77it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 52/65 [00:29<00:07,  1.77it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 53/65 [00:29<00:06,  1.77it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 54/65 [00:30<00:06,  1.77it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 55/65 [00:30<00:05,  1.77it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 56/65 [00:31<00:05,  1.77it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 57/65 [00:32<00:04,  1.78it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 58/65 [00:32<00:03,  1.78it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 59/65 [00:33<00:03,  1.78it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 60/65 [00:33<00:02,  1.78it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 61/65 [00:34<00:02,  1.77it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 62/65 [00:34<00:01,  1.77it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 63/65 [00:35<00:01,  1.77it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 64/65 [00:36<00:00,  1.77it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 65/65 [00:36<00:00,  1.78it/s]\u001b[A\n",
      "Epoch:   1%|          | 2/200 [01:13<2:00:42, 36.58s/it]\n",
      "Iteration:   0%|          | 0/65 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 1/65 [00:00<00:36,  1.76it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 2/65 [00:01<00:35,  1.77it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 3/65 [00:01<00:35,  1.76it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 4/65 [00:02<00:34,  1.77it/s]\u001b[A2021-03-20 07:23:35 - __main__ - INFO: - loss is :6.915833.  global_step:135\n",
      "\n",
      "Iteration:   8%|▊         | 5/65 [00:02<00:33,  1.78it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 6.915833112928603 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   9%|▉         | 6/65 [00:03<00:33,  1.76it/s]\u001b[A\n",
      "Iteration:  11%|█         | 7/65 [00:03<00:32,  1.77it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 8/65 [00:04<00:32,  1.77it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 9/65 [00:05<00:31,  1.77it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 10/65 [00:05<00:31,  1.77it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 11/65 [00:06<00:30,  1.77it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 12/65 [00:06<00:29,  1.78it/s]\u001b[A\n",
      "Iteration:  20%|██        | 13/65 [00:07<00:29,  1.77it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 14/65 [00:07<00:28,  1.77it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 15/65 [00:08<00:28,  1.77it/s]\u001b[A\n",
      "Iteration:  25%|██▍       | 16/65 [00:09<00:27,  1.77it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 17/65 [00:09<00:27,  1.77it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 18/65 [00:10<00:26,  1.76it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 19/65 [00:10<00:26,  1.76it/s]\u001b[A\n",
      "Iteration:  31%|███       | 20/65 [00:11<00:25,  1.76it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 21/65 [00:11<00:24,  1.76it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 22/65 [00:12<00:24,  1.77it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 23/65 [00:13<00:23,  1.77it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 24/65 [00:13<00:23,  1.77it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 25/65 [00:14<00:22,  1.77it/s]\u001b[A\n",
      "Iteration:  40%|████      | 26/65 [00:14<00:21,  1.77it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 27/65 [00:15<00:21,  1.77it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 28/65 [00:15<00:20,  1.77it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 29/65 [00:16<00:20,  1.76it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 30/65 [00:16<00:19,  1.77it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 31/65 [00:17<00:19,  1.77it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 32/65 [00:18<00:18,  1.77it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 33/65 [00:18<00:18,  1.77it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 34/65 [00:19<00:17,  1.77it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 35/65 [00:19<00:16,  1.77it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 36/65 [00:20<00:16,  1.78it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 37/65 [00:20<00:15,  1.77it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 38/65 [00:21<00:15,  1.77it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 39/65 [00:22<00:14,  1.77it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 40/65 [00:22<00:14,  1.78it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 41/65 [00:23<00:13,  1.78it/s]\u001b[A\n",
      "Iteration:  65%|██████▍   | 42/65 [00:23<00:12,  1.77it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 43/65 [00:24<00:12,  1.77it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 44/65 [00:24<00:11,  1.78it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 45/65 [00:25<00:11,  1.78it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 46/65 [00:25<00:10,  1.77it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 47/65 [00:26<00:10,  1.77it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 48/65 [00:27<00:09,  1.77it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 49/65 [00:27<00:09,  1.77it/s]\u001b[A2021-03-20 07:24:00 - __main__ - INFO: - loss is :6.384899.  global_step:180\n",
      "\n",
      "Iteration:  77%|███████▋  | 50/65 [00:28<00:08,  1.77it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 6.384899118211535 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  78%|███████▊  | 51/65 [00:28<00:07,  1.77it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 52/65 [00:29<00:07,  1.78it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 53/65 [00:29<00:06,  1.78it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 54/65 [00:30<00:06,  1.78it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 55/65 [00:31<00:05,  1.78it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 56/65 [00:31<00:05,  1.78it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 57/65 [00:32<00:04,  1.77it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 58/65 [00:32<00:03,  1.77it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 59/65 [00:33<00:03,  1.77it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 60/65 [00:33<00:02,  1.77it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 61/65 [00:34<00:02,  1.77it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 62/65 [00:35<00:01,  1.77it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 63/65 [00:35<00:01,  1.77it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 64/65 [00:36<00:00,  1.77it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 65/65 [00:36<00:00,  1.77it/s]\u001b[A\n",
      "Epoch:   2%|▏         | 3/200 [01:49<2:00:17, 36.64s/it]\n",
      "Iteration:   0%|          | 0/65 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 1/65 [00:00<00:36,  1.77it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPPXkk/IoMfe409MJNx4b3Q",
   "collapsed_sections": [],
   "name": "run_lm_pretraining.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
